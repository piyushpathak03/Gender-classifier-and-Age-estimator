{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9jEfV1BuL-t",
        "colab_type": "text"
      },
      "source": [
        "### Drive Mounter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkX0McPauqiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6a451533-c0da-4711-b222-b54899013334"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrNp05O4uzjJ",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprcessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwf7Lj6Vu4m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fldr=\"drive/My Drive/Face_data_based_ml/CK+48\"\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CPTeRZFvxnM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98109acb-e82e-4f28-e59d-f372f5ad9646"
      },
      "source": [
        "import os\n",
        "files=os.listdir(fldr)\n",
        "print(files)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fear', 'contempt', 'happy', 'anger', 'surprise', 'disgust', 'sadness']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLaJ1-2QwERM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Exp=['fear', 'contempt', 'happy', 'anger', 'surprise', 'disgust', 'sadness']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlQuUFH4wHg9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fcfa4ab-0a5f-42a9-9f14-38bc6eb7f1ab"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "i=0\n",
        "last=[]\n",
        "images=[]\n",
        "labels=[]\n",
        "for fle in files:\n",
        "  idx=Exp.index(fle)\n",
        "  label=idx\n",
        "  \n",
        "  total=fldr+'/'+fle\n",
        "  files_exp= os.listdir(total)\n",
        "\n",
        "  for fle_2 in files_exp:\n",
        "    file_main=total+'/'+fle_2\n",
        "    print(file_main+\"   \"+str(label))\n",
        "    image= cv2.imread(file_main)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image= cv2.resize(image,(48,48))\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "    i+=1\n",
        "  last.append(i)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S084_002_00000022.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S132_003_00000022.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S032_004_00000013.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S054_002_00000015.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S011_003_00000012.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S032_004_00000014.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S504_004_00000014.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S046_003_00000015.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S068_004_00000010.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S050_001_00000016.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S117_003_00000013.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S055_006_00000007.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S074_001_00000019.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S502_004_00000050.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S132_003_00000021.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S091_001_00000015.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S117_003_00000012.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S501_004_00000056.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S050_001_00000015.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S119_003_00000023.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S102_003_00000015.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S091_001_00000014.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S124_003_00000009.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S999_003_00000054.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S138_001_00000011.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S506_004_00000038.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S074_001_00000020.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S011_003_00000014.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S059_002_00000017.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S062_001_00000017.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S102_003_00000014.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S132_003_00000023.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S124_003_00000011.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S502_004_00000052.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S054_002_00000014.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S068_004_00000008.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S124_003_00000010.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S504_004_00000013.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S119_003_00000022.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S084_002_00000021.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S062_001_00000016.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S999_003_00000055.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S032_004_00000012.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S999_003_00000053.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S117_003_00000014.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S050_001_00000017.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S055_006_00000008.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S138_001_00000010.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S084_002_00000023.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S504_004_00000015.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S506_004_00000036.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S102_003_00000016.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S138_001_00000012.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S501_004_00000054.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S501_004_00000055.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S059_002_00000016.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S068_004_00000009.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S054_002_00000013.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S502_004_00000051.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S011_003_00000013.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S091_001_00000013.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S125_006_00000020.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S065_002_00000020.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S125_006_00000022.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S055_006_00000006.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S046_003_00000014.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S074_001_00000018.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S062_001_00000015.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S065_002_00000021.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S046_003_00000016.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S119_003_00000024.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S059_002_00000015.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S506_004_00000037.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S125_006_00000021.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/fear/S065_002_00000022.png   0\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S506_002_00000008.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S504_002_00000008.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S505_002_00000019.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S157_002_00000010.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S895_002_00000006.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S138_008_00000008.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S151_002_00000028.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S149_002_00000011.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S155_002_00000010.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S158_002_00000010.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S139_002_00000011.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S160_006_00000010.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S158_002_00000011.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S502_002_00000008.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S147_002_00000013.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S155_002_00000011.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S147_002_00000012.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S148_002_00000014.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S157_002_00000009.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S504_002_00000007.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S154_002_00000012.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S503_002_00000006.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S156_002_00000020.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S160_006_00000009.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S151_002_00000027.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S505_002_00000020.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S502_002_00000009.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S157_002_00000011.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S147_002_00000011.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S149_002_00000012.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S154_002_00000013.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S138_008_00000009.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S158_002_00000009.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S156_002_00000021.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S502_002_00000007.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S895_002_00000007.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S138_008_00000007.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S155_002_00000012.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S139_002_00000013.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S503_002_00000008.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S506_002_00000009.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S149_002_00000013.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S156_002_00000019.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S154_002_00000011.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S139_002_00000012.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S148_002_00000013.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S503_002_00000007.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S505_002_00000021.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S148_002_00000015.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S160_006_00000008.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S895_002_00000005.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S504_002_00000009.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S506_002_00000007.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/contempt/S151_002_00000029.png   1\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S085_002_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S137_011_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S026_006_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S135_012_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S066_003_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S097_006_00000018.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S135_012_00000018.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S050_006_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S044_003_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S070_003_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S061_002_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S064_003_00000024.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S096_004_00000009.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S115_008_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S091_003_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S037_006_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S067_005_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S056_004_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S100_006_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S078_004_00000026.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S108_008_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S138_005_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S050_006_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S055_005_00000043.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S069_004_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S099_004_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S079_004_00000024.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S099_004_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S091_003_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S138_005_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S132_006_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S076_006_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S052_004_00000031.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S044_003_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S035_006_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S072_006_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S078_004_00000027.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S114_006_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S010_006_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S106_006_00000010.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S109_006_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S075_006_00000024.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S125_005_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S100_006_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S037_006_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S093_004_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S098_004_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S076_006_00000018.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S093_004_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S061_002_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S062_004_00000024.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S115_008_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S116_007_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S114_006_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S091_003_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S134_004_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S089_002_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S053_004_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S131_006_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S072_006_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S055_005_00000044.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S074_005_00000042.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S067_005_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S132_006_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S065_004_00000027.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S079_004_00000025.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S063_002_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S075_006_00000025.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S136_006_00000018.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S096_004_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S131_006_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S011_006_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S011_006_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S026_006_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S060_002_00000026.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S130_013_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S115_008_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S078_004_00000025.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S032_006_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S010_006_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S135_012_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S057_006_00000031.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S053_004_00000024.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S109_006_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S125_005_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S053_004_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S083_003_00000018.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S042_006_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S127_004_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S132_006_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S129_012_00000010.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S057_006_00000032.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S092_004_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S129_012_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S085_002_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S035_006_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S124_007_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S106_006_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S065_004_00000028.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S066_003_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S083_003_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S131_006_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S133_010_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S032_006_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S097_006_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S070_003_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S079_004_00000026.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S069_004_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S096_004_00000010.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S068_002_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S068_002_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S098_004_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S044_003_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S026_006_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S138_005_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S063_002_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S108_008_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S124_007_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S087_005_00000010.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S034_005_00000008.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S097_006_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S087_005_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S064_003_00000025.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S071_005_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S032_006_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S130_013_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S042_006_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S114_006_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S130_013_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S106_006_00000009.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S037_006_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S125_005_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S136_006_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S060_002_00000024.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S124_007_00000024.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S133_010_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S070_003_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S075_006_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S071_005_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S014_005_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S067_005_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S074_005_00000043.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S095_007_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S062_004_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S089_002_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S057_006_00000033.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S014_005_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S056_004_00000018.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S086_002_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S134_004_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S094_004_00000010.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S092_004_00000024.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S092_004_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S071_005_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S137_011_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S056_004_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S095_007_00000020.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S042_006_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S137_011_00000018.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S095_007_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S010_006_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S128_011_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S068_002_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S127_004_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S099_004_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S085_002_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S128_011_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S109_006_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S093_004_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S063_002_00000022.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S034_005_00000009.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S094_004_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S086_002_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S136_006_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S011_006_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S035_006_00000018.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S133_010_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S069_004_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S060_002_00000025.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S074_005_00000041.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S065_004_00000026.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S061_002_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S094_004_00000011.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S087_005_00000012.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S108_008_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S127_004_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S055_005_00000045.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S066_003_00000010.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S076_006_00000019.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S098_004_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S052_004_00000032.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S072_006_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S064_003_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S086_002_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S089_002_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S128_011_00000014.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S116_007_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S100_006_00000015.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S134_004_00000013.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S034_005_00000010.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S050_006_00000021.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S062_004_00000023.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S014_005_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S129_012_00000009.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S083_003_00000017.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S052_004_00000033.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/happy/S116_007_00000016.png   2\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S010_004_00000018.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S011_004_00000020.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S011_004_00000021.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S010_004_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S010_004_00000019.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S011_004_00000019.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S014_003_00000028.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S100_005_00000022.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S058_005_00000010.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S034_003_00000026.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S028_001_00000024.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S032_003_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S037_003_00000020.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S136_005_00000010.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S090_007_00000013.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S126_008_00000029.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S134_003_00000009.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S112_005_00000015.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S072_005_00000018.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S067_004_00000023.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S055_004_00000026.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S066_005_00000009.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S032_003_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S090_007_00000014.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S117_006_00000008.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S029_001_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S092_003_00000013.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S067_004_00000021.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S050_004_00000019.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S092_003_00000014.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S504_001_00000021.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S504_001_00000020.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S029_001_00000018.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S034_003_00000025.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S037_003_00000022.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S087_007_00000015.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S136_005_00000008.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S113_008_00000022.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S072_005_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S089_003_00000035.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S082_005_00000015.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S127_010_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S042_004_00000019.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S503_001_00000070.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S111_006_00000010.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S999_001_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S058_005_00000008.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S082_005_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S072_005_00000019.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S501_001_00000067.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S127_010_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S022_005_00000032.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S506_001_00000038.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S111_006_00000009.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S066_005_00000010.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S026_003_00000015.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S055_004_00000028.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S109_003_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S082_005_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S130_007_00000020.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S503_001_00000071.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S026_003_00000013.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S133_003_00000046.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S100_005_00000021.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S136_005_00000009.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S037_003_00000021.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S119_008_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S111_006_00000008.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S089_003_00000036.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S075_008_00000010.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S045_005_00000028.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S506_001_00000040.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S058_005_00000009.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S087_007_00000014.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S119_008_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S028_001_00000022.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S502_001_00000014.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S503_001_00000069.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S999_001_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S089_003_00000034.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S028_001_00000023.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S999_001_00000018.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S071_004_00000028.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S506_001_00000039.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S045_005_00000029.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S504_001_00000022.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S112_005_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S130_007_00000019.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S066_005_00000011.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S126_008_00000027.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S134_003_00000011.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S090_007_00000012.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S042_004_00000018.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S014_003_00000029.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S501_001_00000066.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S129_006_00000008.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S050_004_00000021.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S133_003_00000045.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S130_007_00000018.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S034_003_00000027.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S129_006_00000009.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S100_005_00000023.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S127_010_00000018.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S071_004_00000027.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S502_001_00000015.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S113_008_00000021.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S133_003_00000047.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S029_001_00000019.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S071_004_00000026.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S092_003_00000012.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S055_004_00000027.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S022_005_00000031.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S042_004_00000020.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S067_004_00000022.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S109_003_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S112_005_00000017.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S117_006_00000009.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S117_006_00000010.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S134_003_00000010.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S075_008_00000011.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S119_008_00000018.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S502_001_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S113_008_00000023.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S045_005_00000030.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S075_008_00000012.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S022_005_00000030.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S026_003_00000014.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S109_003_00000015.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S014_003_00000030.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S050_004_00000020.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S129_006_00000010.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S501_001_00000065.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S087_007_00000016.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S032_003_00000015.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/anger/S126_008_00000028.png   3\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S014_001_00000029.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S057_001_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S010_002_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S063_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S062_002_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S059_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S050_002_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S022_001_00000029.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S026_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S011_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S042_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S056_003_00000008.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S060_003_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S046_002_00000005.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S056_003_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S022_001_00000028.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S046_002_00000004.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S054_003_00000006.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S050_002_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S059_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S010_002_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S054_003_00000005.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S053_001_00000021.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S014_001_00000028.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S057_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S035_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S044_001_00000024.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S056_003_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S052_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S055_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S044_001_00000023.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S052_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S046_002_00000006.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S026_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S022_001_00000030.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S051_002_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S010_002_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S052_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S026_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S011_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S062_002_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S035_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S037_001_00000020.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S062_002_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S054_003_00000007.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S055_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S060_003_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S037_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S014_001_00000027.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S042_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S061_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S055_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S058_001_00000020.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S063_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S051_002_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S034_001_00000029.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S042_001_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S044_001_00000022.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S037_001_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S061_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S053_001_00000022.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S034_001_00000027.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S058_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S050_002_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S059_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S035_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S060_003_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S032_001_00000020.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S011_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S061_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S032_001_00000021.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S058_001_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S051_002_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S053_001_00000023.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S057_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S032_001_00000022.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S063_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S034_001_00000028.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S067_002_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S097_001_00000020.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S094_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S082_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S084_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S078_001_00000031.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S136_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S092_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S132_008_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S113_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S132_008_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S070_002_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S138_004_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S090_002_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S116_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S071_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S126_004_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S133_009_00000005.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S087_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S110_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S075_002_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S122_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S070_002_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S111_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S102_002_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S080_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S073_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S082_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S114_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S107_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S131_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S085_003_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S137_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S137_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S085_003_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S130_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S067_002_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S094_001_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S099_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S075_002_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S068_003_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S065_003_00000021.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S132_008_00000008.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S092_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S064_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S068_003_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S129_002_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S115_001_00000008.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S074_002_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S080_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S088_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S066_002_00000020.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S090_002_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S071_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S081_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S135_001_00000039.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S078_001_00000032.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S073_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S138_004_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S071_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S066_002_00000022.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S111_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S066_002_00000021.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S115_001_00000006.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S110_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S085_003_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S107_001_00000008.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S133_009_00000006.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S107_001_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S117_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S101_002_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S110_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S133_009_00000004.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S086_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S130_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S069_002_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S097_001_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S099_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S096_001_00000005.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S126_004_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S067_002_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S119_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S074_002_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S102_002_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S137_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S115_001_00000007.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S086_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S116_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S125_007_00000008.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S101_002_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S075_002_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S111_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S116_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S069_002_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S073_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S064_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S129_002_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S119_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S125_007_00000007.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S065_003_00000020.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S117_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S135_001_00000037.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S077_001_00000027.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S095_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S088_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S064_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S113_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S089_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S100_002_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S117_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S094_001_00000008.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S096_001_00000006.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S079_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S130_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S076_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S136_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S100_002_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S122_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S136_001_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S087_001_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S087_001_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S135_001_00000038.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S114_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S089_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S082_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S127_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S131_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S095_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S138_004_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S089_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S124_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S127_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S081_001_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S090_002_00000010.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S100_002_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S076_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S126_004_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S084_001_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S079_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S114_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S086_001_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S122_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S102_002_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S084_001_00000008.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S095_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S113_001_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S124_001_00000013.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S124_001_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S081_001_00000019.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S077_001_00000026.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S078_001_00000033.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S097_001_00000021.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S101_002_00000018.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S131_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S079_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S096_001_00000007.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S077_001_00000028.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S074_002_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S080_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S129_002_00000011.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S065_003_00000022.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S092_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S076_001_00000015.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S070_002_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S099_001_00000012.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S068_003_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S127_001_00000017.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S069_002_00000014.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S125_007_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S119_001_00000009.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/surprise/S088_001_00000016.png   4\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S061_004_00000021.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S080_008_00000007.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S062_005_00000029.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S088_004_00000019.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S078_007_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S045_004_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S068_005_00000019.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S054_004_00000023.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S125_008_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S105_008_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S005_001_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S128_004_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S098_003_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S080_008_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S107_005_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S131_010_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S069_003_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S074_004_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S109_005_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S046_004_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S081_008_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S071_006_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S045_004_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S116_006_00000005.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S058_006_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S051_003_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S079_002_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S131_010_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S022_006_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S107_005_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S022_006_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S052_006_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S116_006_00000007.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S035_005_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S097_004_00000029.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S056_002_00000008.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S032_005_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S062_005_00000027.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S105_008_00000008.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S005_001_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S090_006_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S054_004_00000024.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S124_006_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S073_006_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S078_007_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S096_003_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S044_006_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S057_003_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S095_006_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S125_008_00000008.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S082_007_00000008.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S106_004_00000007.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S099_007_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S035_005_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S125_008_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S098_003_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S011_005_00000019.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S087_004_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S079_002_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S051_003_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S106_004_00000006.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S065_005_00000006.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S129_011_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S116_006_00000006.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S077_006_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S107_005_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S097_004_00000030.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S108_006_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S060_005_00000020.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S060_005_00000021.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S081_008_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S099_007_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S128_004_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S076_005_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S128_004_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S061_004_00000022.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S078_007_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S082_007_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S061_004_00000020.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S088_004_00000020.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S069_003_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S069_003_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S095_006_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S057_003_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S044_006_00000019.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S075_005_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S052_006_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S052_006_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S035_005_00000019.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S130_012_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S046_004_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S079_002_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S130_012_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S071_006_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S067_006_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S082_007_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S108_006_00000020.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S071_006_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S056_002_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S062_005_00000028.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S070_005_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S058_006_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S129_011_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S132_005_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S085_004_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S132_005_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S085_004_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S081_008_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S074_004_00000017.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S065_005_00000007.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S102_009_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S102_009_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S075_005_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S109_005_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S134_008_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S070_005_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S095_006_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S096_003_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S124_006_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S080_008_00000008.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S134_008_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S111_007_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S022_006_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S032_005_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S074_004_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S065_005_00000008.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S098_003_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S090_006_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S055_003_00000008.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S130_012_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S077_006_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S055_003_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S099_007_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S111_007_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S090_006_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S068_005_00000020.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S111_007_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S045_004_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S077_006_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S054_004_00000022.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S109_005_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S073_006_00000013.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S108_006_00000019.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S060_005_00000019.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S056_002_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S134_008_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S076_005_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S106_004_00000008.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S067_006_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S011_005_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S051_003_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S044_006_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S088_004_00000018.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S129_011_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S105_008_00000009.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S011_005_00000020.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S073_006_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S097_004_00000028.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S046_004_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S055_003_00000007.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S057_003_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S005_001_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S124_006_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S075_005_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S085_004_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S131_010_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S096_003_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S058_006_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S067_006_00000010.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S132_005_00000016.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S070_005_00000014.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S087_004_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S102_009_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S068_005_00000021.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S087_004_00000011.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S032_005_00000015.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/disgust/S076_005_00000012.png   5\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S081_002_00000022.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S026_002_00000015.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S026_002_00000016.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S131_003_00000023.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S504_006_00000018.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S011_002_00000021.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S042_002_00000015.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S503_006_00000018.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S042_002_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S066_004_00000008.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S136_003_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S080_005_00000013.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S108_005_00000021.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S064_004_00000013.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S080_005_00000011.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S108_005_00000022.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S506_006_00000040.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S026_002_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S042_002_00000016.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S137_005_00000026.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S066_004_00000009.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S130_009_00000018.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S132_002_00000016.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S093_001_00000018.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S501_006_00000039.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S106_002_00000016.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S505_006_00000019.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S046_001_00000024.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S504_006_00000016.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S046_001_00000025.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S501_006_00000040.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S130_009_00000019.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S132_002_00000018.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S113_003_00000013.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S137_005_00000025.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S131_003_00000024.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S095_010_00000012.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S066_004_00000010.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S115_004_00000016.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S506_006_00000041.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S503_006_00000020.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S132_002_00000017.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S138_007_00000010.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S071_002_00000018.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S093_001_00000020.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S108_005_00000020.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S071_002_00000020.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S106_002_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S115_004_00000017.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S064_004_00000012.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S130_009_00000017.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S136_003_00000012.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S125_001_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S137_005_00000027.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S113_003_00000015.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S014_002_00000015.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S125_001_00000012.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S081_002_00000024.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S131_003_00000022.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S504_006_00000017.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S505_006_00000018.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S095_010_00000013.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S503_006_00000019.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S106_002_00000015.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S011_002_00000020.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S080_005_00000012.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S014_002_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S125_001_00000013.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S014_002_00000016.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S081_002_00000023.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S064_004_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S046_001_00000023.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S113_003_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S011_002_00000022.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S501_006_00000041.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S115_004_00000015.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S093_001_00000019.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S138_007_00000011.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S071_002_00000019.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S095_010_00000014.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S138_007_00000009.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S505_006_00000017.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S506_006_00000042.png   6\n",
            "drive/My Drive/Face_data_based_ml/CK+48/sadness/S136_003_00000013.png   6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBjyfD5245gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRdHutrn48Q8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b0ac0e6-9d7a-4d6d-daa9-c7dcd93f90ce"
      },
      "source": [
        "last"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[75, 129, 336, 471, 720, 897, 981]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm8OGs725qC0",
        "colab_type": "text"
      },
      "source": [
        "#### Fear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQeMquTS5CLY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "2663f353-0a04-4b05-f784-773ff7fbc30e"
      },
      "source": [
        "cv2_imshow(images[24])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9FCFA8EDA0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B88tJpz65aIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "0b86f4f6-0e4e-4837-8a6d-c9a63192305a"
      },
      "source": [
        "cv2_imshow(images[40])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9FCFA8EDD8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkzhBnKj5tu5",
        "colab_type": "text"
      },
      "source": [
        "#### Contempt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-k3qsf-5eEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "49c0fc2d-f9b3-4d5c-ee24-84636c048de1"
      },
      "source": [
        "cv2_imshow(images[82])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9FC59974E0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWnfH_p_5zGs",
        "colab_type": "text"
      },
      "source": [
        "#### Happy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4od79JNf5wXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "b13d0072-a355-4279-e5d2-02101124583f"
      },
      "source": [
        "cv2_imshow(images[200])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9FC5997978>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH0H70hf58OZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "023c5f82-fa7c-4c80-c15d-4fd3ed4da237"
      },
      "source": [
        "cv2_imshow(images[300])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9FC59979B0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPDUU8eI7CRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AazMQza7-KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "images_f=np.array(images)\n",
        "labels_f=np.array(labels)\n",
        "\n",
        "images_f_2=images_f/255\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3aEjV3oFGKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "9eacc8c6-9963-4e9b-d5fc-b7a9abf6a2c9"
      },
      "source": [
        "cv2_imshow(images[300])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F7C57ED68>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urhM93muFJE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2587ab1-ea4e-4189-861f-74fb3800ca05"
      },
      "source": [
        "images_f_2.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(981, 48, 48, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XDADyGeFNEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_of_classes=7\n",
        "labels_encoded=tf.keras.utils.to_categorical(labels_f,num_classes=num_of_classes)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BOhWJ44FvH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test= train_test_split(images_f_2, labels_encoded,test_size=0.25)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwP_RGvUGt0h",
        "colab_type": "text"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YESdhmIpGKAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten,BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D,Conv2D\n",
        "from tensorflow.keras.layers import Input,Activation,Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def Convolution(input_tensor,filters):\n",
        "    \n",
        "    x = Conv2D(filters=filters,kernel_size=(3, 3),padding = 'same',strides=(1, 1),kernel_regularizer=l2(0.001))(input_tensor)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x= Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "def model(input_shape):\n",
        "  inputs = Input((input_shape))\n",
        "  \n",
        "  conv_1= Convolution(inputs,32)\n",
        "  maxp_1 = MaxPooling2D(pool_size = (2,2)) (conv_1)\n",
        "  conv_2 = Convolution(maxp_1,64)\n",
        "  maxp_2 = MaxPooling2D(pool_size = (2, 2)) (conv_2)\n",
        "  conv_3 = Convolution(maxp_2,128)\n",
        "  maxp_3 = MaxPooling2D(pool_size = (2, 2)) (conv_3)\n",
        "  conv_4 = Convolution(maxp_3,256)\n",
        "  maxp_4 = MaxPooling2D(pool_size = (2, 2)) (conv_4)\n",
        "  flatten= Flatten() (maxp_4)\n",
        "  dense_1= Dense(128,activation='relu')(flatten)\n",
        "  drop_1=Dropout(0.2)(dense_1)\n",
        "  output= Dense(7,activation=\"sigmoid\")(drop_1)\n",
        "\n",
        "  model = Model(inputs=[inputs], outputs=[output])\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\",\n",
        "\tmetrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6_MmJqTGb4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Model=model(input_shape = (48,48,3))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP8Y6mNwGl-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "8e324747-cbb9-46e5-98b8-72a48495f1d8"
      },
      "source": [
        "Model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 48, 48, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 6, 256)         295168    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               295040    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 684,359\n",
            "Trainable params: 684,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em_s679QGyOs",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPf8o5TRGpZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltft2Z6bG2eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fle_s='Emotion_detection.h5'\n",
        "checkpointer = ModelCheckpoint(fle_s, monitor='loss',verbose=1,save_best_only=True,save_weights_only=False, mode='auto',save_freq='epoch')\n",
        "callback_list=[checkpointer]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf9ZgmksG96G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db1ad5e0-6a8b-47c1-e678-09297b61d698"
      },
      "source": [
        "History=Model.fit(X_train,Y_train,batch_size=32,validation_data=(X_test,Y_test),epochs=1000,callbacks=[callback_list])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 2.1007 - accuracy: 0.1891\n",
            "Epoch 00001: loss improved from inf to 2.10071, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 2.1007 - accuracy: 0.1891 - val_loss: 1.9984 - val_accuracy: 0.2846\n",
            "Epoch 2/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 1.9562 - accuracy: 0.2485\n",
            "Epoch 00002: loss improved from 2.10071 to 1.95513, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.9551 - accuracy: 0.2476 - val_loss: 1.9785 - val_accuracy: 0.2846\n",
            "Epoch 3/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 1.9080 - accuracy: 0.2500\n",
            "Epoch 00003: loss improved from 1.95513 to 1.90576, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.9058 - accuracy: 0.2558 - val_loss: 1.8520 - val_accuracy: 0.2846\n",
            "Epoch 4/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 1.8212 - accuracy: 0.2937\n",
            "Epoch 00004: loss improved from 1.90576 to 1.81039, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 1.8104 - accuracy: 0.3156 - val_loss: 1.7138 - val_accuracy: 0.4715\n",
            "Epoch 5/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 1.4922 - accuracy: 0.4271\n",
            "Epoch 00005: loss improved from 1.81039 to 1.47417, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.4742 - accuracy: 0.4313 - val_loss: 1.3483 - val_accuracy: 0.5081\n",
            "Epoch 6/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 1.2905 - accuracy: 0.5193\n",
            "Epoch 00006: loss improved from 1.47417 to 1.29005, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.2901 - accuracy: 0.5197 - val_loss: 1.0869 - val_accuracy: 0.6504\n",
            "Epoch 7/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 1.0816 - accuracy: 0.6057\n",
            "Epoch 00007: loss improved from 1.29005 to 1.07102, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 1.0710 - accuracy: 0.6082 - val_loss: 0.9420 - val_accuracy: 0.6463\n",
            "Epoch 8/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.8249 - accuracy: 0.7128\n",
            "Epoch 00008: loss improved from 1.07102 to 0.82896, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.8290 - accuracy: 0.7156 - val_loss: 0.7792 - val_accuracy: 0.8577\n",
            "Epoch 9/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.6715 - accuracy: 0.7976\n",
            "Epoch 00009: loss improved from 0.82896 to 0.66453, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.6645 - accuracy: 0.7986 - val_loss: 0.5984 - val_accuracy: 0.8130\n",
            "Epoch 10/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.5195 - accuracy: 0.8542\n",
            "Epoch 00010: loss improved from 0.66453 to 0.51380, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.5138 - accuracy: 0.8544 - val_loss: 0.4032 - val_accuracy: 0.9228\n",
            "Epoch 11/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.4304 - accuracy: 0.8929\n",
            "Epoch 00011: loss improved from 0.51380 to 0.41970, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4197 - accuracy: 0.8993 - val_loss: 0.4815 - val_accuracy: 0.8537\n",
            "Epoch 12/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.4161 - accuracy: 0.8973\n",
            "Epoch 00012: loss improved from 0.41970 to 0.41308, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.4131 - accuracy: 0.8980 - val_loss: 0.3455 - val_accuracy: 0.9512\n",
            "Epoch 13/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.3699 - accuracy: 0.9137\n",
            "Epoch 00013: loss improved from 0.41308 to 0.37983, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3798 - accuracy: 0.9102 - val_loss: 0.3962 - val_accuracy: 0.9390\n",
            "Epoch 14/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.3152 - accuracy: 0.9390\n",
            "Epoch 00014: loss improved from 0.37983 to 0.33958, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3396 - accuracy: 0.9306 - val_loss: 0.3249 - val_accuracy: 0.9593\n",
            "Epoch 15/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.3101 - accuracy: 0.9345\n",
            "Epoch 00015: loss improved from 0.33958 to 0.30825, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3083 - accuracy: 0.9347 - val_loss: 0.2571 - val_accuracy: 0.9675\n",
            "Epoch 16/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2966 - accuracy: 0.9479\n",
            "Epoch 00016: loss improved from 0.30825 to 0.30331, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.3033 - accuracy: 0.9442 - val_loss: 0.3291 - val_accuracy: 0.9309\n",
            "Epoch 17/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2730 - accuracy: 0.9494\n",
            "Epoch 00017: loss improved from 0.30331 to 0.26800, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2680 - accuracy: 0.9524 - val_loss: 0.2474 - val_accuracy: 0.9715\n",
            "Epoch 18/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2602 - accuracy: 0.9583\n",
            "Epoch 00018: loss improved from 0.26800 to 0.26428, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2643 - accuracy: 0.9578 - val_loss: 0.2635 - val_accuracy: 0.9512\n",
            "Epoch 19/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2269 - accuracy: 0.9672\n",
            "Epoch 00019: loss improved from 0.26428 to 0.22163, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2216 - accuracy: 0.9701 - val_loss: 0.2221 - val_accuracy: 0.9675\n",
            "Epoch 20/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2075 - accuracy: 0.9747\n",
            "Epoch 00020: loss improved from 0.22163 to 0.20913, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2091 - accuracy: 0.9728 - val_loss: 0.2592 - val_accuracy: 0.9553\n",
            "Epoch 21/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1994 - accuracy: 0.9851\n",
            "Epoch 00021: loss improved from 0.20913 to 0.19656, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1966 - accuracy: 0.9850 - val_loss: 0.2010 - val_accuracy: 0.9756\n",
            "Epoch 22/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2229 - accuracy: 0.9643\n",
            "Epoch 00022: loss did not improve from 0.19656\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2179 - accuracy: 0.9673 - val_loss: 0.2504 - val_accuracy: 0.9634\n",
            "Epoch 23/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2179 - accuracy: 0.9732\n",
            "Epoch 00023: loss did not improve from 0.19656\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2175 - accuracy: 0.9728 - val_loss: 0.1829 - val_accuracy: 1.0000\n",
            "Epoch 24/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1786 - accuracy: 0.9881\n",
            "Epoch 00024: loss improved from 0.19656 to 0.17722, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1772 - accuracy: 0.9878 - val_loss: 0.1700 - val_accuracy: 0.9959\n",
            "Epoch 25/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1827 - accuracy: 0.9821\n",
            "Epoch 00025: loss did not improve from 0.17722\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1833 - accuracy: 0.9823 - val_loss: 0.1832 - val_accuracy: 0.9878\n",
            "Epoch 26/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1814 - accuracy: 0.9777\n",
            "Epoch 00026: loss did not improve from 0.17722\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1845 - accuracy: 0.9755 - val_loss: 0.2241 - val_accuracy: 0.9553\n",
            "Epoch 27/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1921 - accuracy: 0.9777\n",
            "Epoch 00027: loss did not improve from 0.17722\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1889 - accuracy: 0.9796 - val_loss: 0.2052 - val_accuracy: 0.9715\n",
            "Epoch 28/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1797 - accuracy: 0.9777\n",
            "Epoch 00028: loss did not improve from 0.17722\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1790 - accuracy: 0.9796 - val_loss: 0.1994 - val_accuracy: 0.9715\n",
            "Epoch 29/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1766 - accuracy: 0.9836\n",
            "Epoch 00029: loss improved from 0.17722 to 0.17588, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1759 - accuracy: 0.9837 - val_loss: 0.1634 - val_accuracy: 1.0000\n",
            "Epoch 30/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1542 - accuracy: 0.9881\n",
            "Epoch 00030: loss improved from 0.17588 to 0.15435, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.1543 - accuracy: 0.9878 - val_loss: 0.1696 - val_accuracy: 0.9837\n",
            "Epoch 31/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1403 - accuracy: 0.9970\n",
            "Epoch 00031: loss improved from 0.15435 to 0.14333, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1433 - accuracy: 0.9959 - val_loss: 0.1350 - val_accuracy: 1.0000\n",
            "Epoch 32/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1569 - accuracy: 0.9851\n",
            "Epoch 00032: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1639 - accuracy: 0.9837 - val_loss: 0.2032 - val_accuracy: 0.9878\n",
            "Epoch 33/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1563 - accuracy: 0.9866\n",
            "Epoch 00033: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1538 - accuracy: 0.9878 - val_loss: 0.1532 - val_accuracy: 0.9959\n",
            "Epoch 34/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1437 - accuracy: 0.9896\n",
            "Epoch 00034: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1469 - accuracy: 0.9891 - val_loss: 0.1587 - val_accuracy: 0.9837\n",
            "Epoch 35/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1457 - accuracy: 0.9866\n",
            "Epoch 00035: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1461 - accuracy: 0.9864 - val_loss: 0.2090 - val_accuracy: 0.9715\n",
            "Epoch 36/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1822 - accuracy: 0.9762\n",
            "Epoch 00036: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1785 - accuracy: 0.9782 - val_loss: 0.1458 - val_accuracy: 0.9959\n",
            "Epoch 37/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1424 - accuracy: 0.9911\n",
            "Epoch 00037: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1454 - accuracy: 0.9891 - val_loss: 0.1718 - val_accuracy: 0.9797\n",
            "Epoch 38/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1403 - accuracy: 0.9911\n",
            "Epoch 00038: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1458 - accuracy: 0.9891 - val_loss: 0.1372 - val_accuracy: 1.0000\n",
            "Epoch 39/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1502 - accuracy: 0.9881\n",
            "Epoch 00039: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1484 - accuracy: 0.9891 - val_loss: 0.1671 - val_accuracy: 0.9837\n",
            "Epoch 40/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1565 - accuracy: 0.9821\n",
            "Epoch 00040: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1588 - accuracy: 0.9810 - val_loss: 0.1713 - val_accuracy: 0.9797\n",
            "Epoch 41/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2019 - accuracy: 0.9732\n",
            "Epoch 00041: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2015 - accuracy: 0.9741 - val_loss: 0.1991 - val_accuracy: 0.9756\n",
            "Epoch 42/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1701 - accuracy: 0.9844\n",
            "Epoch 00042: loss did not improve from 0.14333\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1676 - accuracy: 0.9850 - val_loss: 0.1550 - val_accuracy: 0.9919\n",
            "Epoch 43/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1398 - accuracy: 0.9955\n",
            "Epoch 00043: loss improved from 0.14333 to 0.14058, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1406 - accuracy: 0.9946 - val_loss: 0.1684 - val_accuracy: 0.9878\n",
            "Epoch 44/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1413 - accuracy: 0.9926\n",
            "Epoch 00044: loss improved from 0.14058 to 0.13957, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1396 - accuracy: 0.9932 - val_loss: 0.1385 - val_accuracy: 1.0000\n",
            "Epoch 45/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1322 - accuracy: 0.9970\n",
            "Epoch 00045: loss improved from 0.13957 to 0.13582, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1358 - accuracy: 0.9946 - val_loss: 0.1602 - val_accuracy: 0.9878\n",
            "Epoch 46/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1284 - accuracy: 0.9953\n",
            "Epoch 00046: loss improved from 0.13582 to 0.12938, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1294 - accuracy: 0.9946 - val_loss: 0.1275 - val_accuracy: 0.9919\n",
            "Epoch 47/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1206 - accuracy: 0.9985\n",
            "Epoch 00047: loss improved from 0.12938 to 0.12154, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1215 - accuracy: 0.9973 - val_loss: 0.1297 - val_accuracy: 0.9919\n",
            "Epoch 48/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1318 - accuracy: 0.9911\n",
            "Epoch 00048: loss did not improve from 0.12154\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1305 - accuracy: 0.9918 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
            "Epoch 49/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1112 - accuracy: 0.9985\n",
            "Epoch 00049: loss improved from 0.12154 to 0.11397, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1140 - accuracy: 0.9959 - val_loss: 0.1111 - val_accuracy: 1.0000\n",
            "Epoch 50/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1173 - accuracy: 0.9955\n",
            "Epoch 00050: loss did not improve from 0.11397\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1161 - accuracy: 0.9959 - val_loss: 0.1109 - val_accuracy: 1.0000\n",
            "Epoch 51/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1119 - accuracy: 0.9985\n",
            "Epoch 00051: loss improved from 0.11397 to 0.11224, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1122 - accuracy: 0.9986 - val_loss: 0.1159 - val_accuracy: 0.9959\n",
            "Epoch 52/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1056 - accuracy: 0.9985\n",
            "Epoch 00052: loss improved from 0.11224 to 0.10501, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1050 - accuracy: 0.9986 - val_loss: 0.1083 - val_accuracy: 0.9959\n",
            "Epoch 53/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0999 - accuracy: 1.0000\n",
            "Epoch 00053: loss improved from 0.10501 to 0.09946, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0995 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 0.9959\n",
            "Epoch 54/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0958 - accuracy: 1.0000\n",
            "Epoch 00054: loss improved from 0.09946 to 0.09600, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0960 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 1.0000\n",
            "Epoch 55/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1142 - accuracy: 0.9896\n",
            "Epoch 00055: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1249 - accuracy: 0.9864 - val_loss: 0.1594 - val_accuracy: 0.9675\n",
            "Epoch 56/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1350 - accuracy: 0.9881\n",
            "Epoch 00056: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1334 - accuracy: 0.9891 - val_loss: 0.1241 - val_accuracy: 1.0000\n",
            "Epoch 57/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1113 - accuracy: 0.9955\n",
            "Epoch 00057: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1110 - accuracy: 0.9959 - val_loss: 0.1060 - val_accuracy: 0.9959\n",
            "Epoch 58/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1024 - accuracy: 0.9970\n",
            "Epoch 00058: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1069 - accuracy: 0.9959 - val_loss: 0.1077 - val_accuracy: 0.9959\n",
            "Epoch 59/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1179 - accuracy: 0.9953\n",
            "Epoch 00059: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1168 - accuracy: 0.9946 - val_loss: 0.1094 - val_accuracy: 1.0000\n",
            "Epoch 60/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1064 - accuracy: 0.9926\n",
            "Epoch 00060: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1159 - accuracy: 0.9891 - val_loss: 0.1095 - val_accuracy: 1.0000\n",
            "Epoch 61/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1114 - accuracy: 0.9940\n",
            "Epoch 00061: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1112 - accuracy: 0.9946 - val_loss: 0.1504 - val_accuracy: 0.9756\n",
            "Epoch 62/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1427 - accuracy: 0.9866\n",
            "Epoch 00062: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1398 - accuracy: 0.9878 - val_loss: 0.1420 - val_accuracy: 0.9837\n",
            "Epoch 63/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1362 - accuracy: 0.9851\n",
            "Epoch 00063: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1345 - accuracy: 0.9864 - val_loss: 0.1212 - val_accuracy: 0.9919\n",
            "Epoch 64/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1051 - accuracy: 1.0000\n",
            "Epoch 00064: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1062 - accuracy: 0.9986 - val_loss: 0.1101 - val_accuracy: 0.9919\n",
            "Epoch 65/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1313 - accuracy: 0.9896\n",
            "Epoch 00065: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1294 - accuracy: 0.9891 - val_loss: 0.1144 - val_accuracy: 1.0000\n",
            "Epoch 66/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1068 - accuracy: 0.9985\n",
            "Epoch 00066: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1096 - accuracy: 0.9959 - val_loss: 0.1052 - val_accuracy: 0.9959\n",
            "Epoch 67/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1131 - accuracy: 0.9955\n",
            "Epoch 00067: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1117 - accuracy: 0.9959 - val_loss: 0.1257 - val_accuracy: 0.9837\n",
            "Epoch 68/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1099 - accuracy: 0.9955\n",
            "Epoch 00068: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1130 - accuracy: 0.9946 - val_loss: 0.1410 - val_accuracy: 0.9837\n",
            "Epoch 69/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1687 - accuracy: 0.9792\n",
            "Epoch 00069: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1688 - accuracy: 0.9782 - val_loss: 0.2089 - val_accuracy: 0.9756\n",
            "Epoch 70/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1457 - accuracy: 0.9911\n",
            "Epoch 00070: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1443 - accuracy: 0.9905 - val_loss: 0.1436 - val_accuracy: 0.9878\n",
            "Epoch 71/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1438 - accuracy: 0.9836\n",
            "Epoch 00071: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1476 - accuracy: 0.9823 - val_loss: 0.1414 - val_accuracy: 0.9959\n",
            "Epoch 72/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1573 - accuracy: 0.9792\n",
            "Epoch 00072: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1568 - accuracy: 0.9796 - val_loss: 0.1534 - val_accuracy: 0.9878\n",
            "Epoch 73/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1334 - accuracy: 0.9926\n",
            "Epoch 00073: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1311 - accuracy: 0.9932 - val_loss: 0.1403 - val_accuracy: 0.9919\n",
            "Epoch 74/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1227 - accuracy: 0.9955\n",
            "Epoch 00074: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1219 - accuracy: 0.9959 - val_loss: 0.1201 - val_accuracy: 0.9959\n",
            "Epoch 75/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1093 - accuracy: 0.9955\n",
            "Epoch 00075: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1087 - accuracy: 0.9959 - val_loss: 0.1067 - val_accuracy: 1.0000\n",
            "Epoch 76/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1101 - accuracy: 0.9955\n",
            "Epoch 00076: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1098 - accuracy: 0.9959 - val_loss: 0.1108 - val_accuracy: 0.9959\n",
            "Epoch 77/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1112 - accuracy: 0.9970\n",
            "Epoch 00077: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1200 - accuracy: 0.9959 - val_loss: 0.1167 - val_accuracy: 0.9959\n",
            "Epoch 78/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1109 - accuracy: 0.9985\n",
            "Epoch 00078: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1098 - accuracy: 0.9986 - val_loss: 0.1010 - val_accuracy: 1.0000\n",
            "Epoch 79/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1135 - accuracy: 0.9926\n",
            "Epoch 00079: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1120 - accuracy: 0.9932 - val_loss: 0.1064 - val_accuracy: 0.9959\n",
            "Epoch 80/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1059 - accuracy: 0.9940\n",
            "Epoch 00080: loss did not improve from 0.09600\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1057 - accuracy: 0.9946 - val_loss: 0.1027 - val_accuracy: 1.0000\n",
            "Epoch 81/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0947 - accuracy: 1.0000\n",
            "Epoch 00081: loss improved from 0.09600 to 0.09476, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0948 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9959\n",
            "Epoch 82/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1099 - accuracy: 0.9926\n",
            "Epoch 00082: loss did not improve from 0.09476\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1087 - accuracy: 0.9932 - val_loss: 0.1214 - val_accuracy: 0.9959\n",
            "Epoch 83/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1041 - accuracy: 0.9922\n",
            "Epoch 00083: loss did not improve from 0.09476\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1022 - accuracy: 0.9932 - val_loss: 0.0967 - val_accuracy: 1.0000\n",
            "Epoch 84/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0914 - accuracy: 0.9985\n",
            "Epoch 00084: loss improved from 0.09476 to 0.09098, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0910 - accuracy: 0.9986 - val_loss: 0.0920 - val_accuracy: 1.0000\n",
            "Epoch 85/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0874 - accuracy: 0.9985\n",
            "Epoch 00085: loss improved from 0.09098 to 0.08738, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0874 - accuracy: 0.9986 - val_loss: 0.0880 - val_accuracy: 1.0000\n",
            "Epoch 86/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0846 - accuracy: 0.9985\n",
            "Epoch 00086: loss improved from 0.08738 to 0.08690, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0869 - accuracy: 0.9973 - val_loss: 0.0921 - val_accuracy: 0.9959\n",
            "Epoch 87/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0951 - accuracy: 0.9969\n",
            "Epoch 00087: loss did not improve from 0.08690\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0935 - accuracy: 0.9973 - val_loss: 0.0881 - val_accuracy: 1.0000\n",
            "Epoch 88/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0862 - accuracy: 0.9970\n",
            "Epoch 00088: loss improved from 0.08690 to 0.08562, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0856 - accuracy: 0.9973 - val_loss: 0.0870 - val_accuracy: 1.0000\n",
            "Epoch 89/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0900 - accuracy: 0.9955\n",
            "Epoch 00089: loss did not improve from 0.08562\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0900 - accuracy: 0.9959 - val_loss: 0.1160 - val_accuracy: 0.9878\n",
            "Epoch 90/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0879 - accuracy: 0.9940\n",
            "Epoch 00090: loss did not improve from 0.08562\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0923 - accuracy: 0.9918 - val_loss: 0.0882 - val_accuracy: 1.0000\n",
            "Epoch 91/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0989 - accuracy: 0.9970\n",
            "Epoch 00091: loss did not improve from 0.08562\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1065 - accuracy: 0.9946 - val_loss: 0.1177 - val_accuracy: 0.9919\n",
            "Epoch 92/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1172 - accuracy: 0.9836\n",
            "Epoch 00092: loss did not improve from 0.08562\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1209 - accuracy: 0.9810 - val_loss: 0.0987 - val_accuracy: 0.9959\n",
            "Epoch 93/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1103 - accuracy: 0.9940\n",
            "Epoch 00093: loss did not improve from 0.08562\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1110 - accuracy: 0.9932 - val_loss: 0.1040 - val_accuracy: 0.9959\n",
            "Epoch 94/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0931 - accuracy: 0.9955\n",
            "Epoch 00094: loss did not improve from 0.08562\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0924 - accuracy: 0.9959 - val_loss: 0.0902 - val_accuracy: 0.9959\n",
            "Epoch 95/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0852 - accuracy: 0.9985\n",
            "Epoch 00095: loss improved from 0.08562 to 0.08471, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.0847 - accuracy: 0.9986 - val_loss: 0.0840 - val_accuracy: 1.0000\n",
            "Epoch 96/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0834 - accuracy: 0.9984\n",
            "Epoch 00096: loss improved from 0.08471 to 0.08288, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0829 - accuracy: 0.9986 - val_loss: 0.0875 - val_accuracy: 0.9959\n",
            "Epoch 97/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0802 - accuracy: 0.9985\n",
            "Epoch 00097: loss improved from 0.08288 to 0.08049, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0805 - accuracy: 0.9986 - val_loss: 0.0901 - val_accuracy: 0.9959\n",
            "Epoch 98/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0975 - accuracy: 0.9911\n",
            "Epoch 00098: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0960 - accuracy: 0.9918 - val_loss: 0.0960 - val_accuracy: 0.9959\n",
            "Epoch 99/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0877 - accuracy: 0.9970\n",
            "Epoch 00099: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0875 - accuracy: 0.9973 - val_loss: 0.1123 - val_accuracy: 0.9878\n",
            "Epoch 100/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1074 - accuracy: 0.9896\n",
            "Epoch 00100: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1141 - accuracy: 0.9878 - val_loss: 0.1053 - val_accuracy: 0.9959\n",
            "Epoch 101/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0955 - accuracy: 0.9940\n",
            "Epoch 00101: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0981 - accuracy: 0.9932 - val_loss: 0.0900 - val_accuracy: 0.9959\n",
            "Epoch 102/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0890 - accuracy: 0.9969\n",
            "Epoch 00102: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0885 - accuracy: 0.9973 - val_loss: 0.0895 - val_accuracy: 1.0000\n",
            "Epoch 103/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0827 - accuracy: 0.9969\n",
            "Epoch 00103: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0816 - accuracy: 0.9973 - val_loss: 0.0789 - val_accuracy: 1.0000\n",
            "Epoch 104/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0807 - accuracy: 0.9970\n",
            "Epoch 00104: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0809 - accuracy: 0.9973 - val_loss: 0.1900 - val_accuracy: 0.9675\n",
            "Epoch 105/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0969 - accuracy: 0.9937\n",
            "Epoch 00105: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0942 - accuracy: 0.9946 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
            "Epoch 106/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9946\n",
            "Epoch 00106: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1038 - accuracy: 0.9946 - val_loss: 0.1242 - val_accuracy: 0.9878\n",
            "Epoch 107/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1042 - accuracy: 0.9911\n",
            "Epoch 00107: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1083 - accuracy: 0.9878 - val_loss: 0.0953 - val_accuracy: 0.9959\n",
            "Epoch 108/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1103 - accuracy: 0.9940\n",
            "Epoch 00108: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1085 - accuracy: 0.9946 - val_loss: 0.0953 - val_accuracy: 0.9959\n",
            "Epoch 109/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0937 - accuracy: 0.9940\n",
            "Epoch 00109: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0944 - accuracy: 0.9946 - val_loss: 0.1131 - val_accuracy: 0.9959\n",
            "Epoch 110/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0907 - accuracy: 0.9985\n",
            "Epoch 00110: loss did not improve from 0.08049\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0900 - accuracy: 0.9986 - val_loss: 0.0814 - val_accuracy: 1.0000\n",
            "Epoch 111/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0805 - accuracy: 0.9985\n",
            "Epoch 00111: loss improved from 0.08049 to 0.08019, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0802 - accuracy: 0.9986 - val_loss: 0.0867 - val_accuracy: 0.9959\n",
            "Epoch 112/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9905\n",
            "Epoch 00112: loss did not improve from 0.08019\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1027 - accuracy: 0.9905 - val_loss: 0.1286 - val_accuracy: 0.9837\n",
            "Epoch 113/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0849 - accuracy: 1.0000\n",
            "Epoch 00113: loss did not improve from 0.08019\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0867 - accuracy: 0.9986 - val_loss: 0.0806 - val_accuracy: 1.0000\n",
            "Epoch 114/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0862 - accuracy: 0.9970\n",
            "Epoch 00114: loss did not improve from 0.08019\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0860 - accuracy: 0.9973 - val_loss: 0.0856 - val_accuracy: 0.9959\n",
            "Epoch 115/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0781 - accuracy: 0.9984\n",
            "Epoch 00115: loss improved from 0.08019 to 0.07771, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0777 - accuracy: 0.9986 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
            "Epoch 116/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1140 - accuracy: 0.9940\n",
            "Epoch 00116: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1141 - accuracy: 0.9946 - val_loss: 0.1160 - val_accuracy: 0.9959\n",
            "Epoch 117/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0971 - accuracy: 0.9937\n",
            "Epoch 00117: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0963 - accuracy: 0.9932 - val_loss: 0.0996 - val_accuracy: 0.9919\n",
            "Epoch 118/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0932 - accuracy: 0.9926\n",
            "Epoch 00118: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0919 - accuracy: 0.9932 - val_loss: 0.1006 - val_accuracy: 0.9919\n",
            "Epoch 119/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9918\n",
            "Epoch 00119: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0930 - accuracy: 0.9918 - val_loss: 0.1049 - val_accuracy: 0.9919\n",
            "Epoch 120/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0850 - accuracy: 0.9955\n",
            "Epoch 00120: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0848 - accuracy: 0.9959 - val_loss: 0.0839 - val_accuracy: 1.0000\n",
            "Epoch 121/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1447 - accuracy: 0.9717\n",
            "Epoch 00121: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1466 - accuracy: 0.9701 - val_loss: 0.1905 - val_accuracy: 0.9593\n",
            "Epoch 122/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1316 - accuracy: 0.9866\n",
            "Epoch 00122: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1296 - accuracy: 0.9878 - val_loss: 0.1187 - val_accuracy: 0.9878\n",
            "Epoch 123/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0931 - accuracy: 0.9970\n",
            "Epoch 00123: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0948 - accuracy: 0.9959 - val_loss: 0.1208 - val_accuracy: 0.9878\n",
            "Epoch 124/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1806 - accuracy: 0.9734\n",
            "Epoch 00124: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1731 - accuracy: 0.9769 - val_loss: 0.1409 - val_accuracy: 0.9959\n",
            "Epoch 125/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1101 - accuracy: 0.9940\n",
            "Epoch 00125: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1121 - accuracy: 0.9932 - val_loss: 0.1002 - val_accuracy: 1.0000\n",
            "Epoch 126/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1257 - accuracy: 0.9896\n",
            "Epoch 00126: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1229 - accuracy: 0.9905 - val_loss: 0.1196 - val_accuracy: 0.9959\n",
            "Epoch 127/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1201 - accuracy: 0.9866\n",
            "Epoch 00127: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1185 - accuracy: 0.9878 - val_loss: 0.1037 - val_accuracy: 0.9919\n",
            "Epoch 128/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0969 - accuracy: 0.9985\n",
            "Epoch 00128: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0971 - accuracy: 0.9986 - val_loss: 0.0959 - val_accuracy: 0.9959\n",
            "Epoch 129/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0973 - accuracy: 0.9955\n",
            "Epoch 00129: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0967 - accuracy: 0.9959 - val_loss: 0.0958 - val_accuracy: 1.0000\n",
            "Epoch 130/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0969 - accuracy: 0.9955\n",
            "Epoch 00130: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0980 - accuracy: 0.9946 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
            "Epoch 131/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0894 - accuracy: 0.9970\n",
            "Epoch 00131: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0889 - accuracy: 0.9973 - val_loss: 0.0848 - val_accuracy: 1.0000\n",
            "Epoch 132/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0867 - accuracy: 0.9985\n",
            "Epoch 00132: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0863 - accuracy: 0.9986 - val_loss: 0.0996 - val_accuracy: 0.9959\n",
            "Epoch 133/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0973 - accuracy: 0.9970\n",
            "Epoch 00133: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0961 - accuracy: 0.9973 - val_loss: 0.1074 - val_accuracy: 0.9959\n",
            "Epoch 134/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0836 - accuracy: 1.0000\n",
            "Epoch 00134: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0832 - accuracy: 1.0000 - val_loss: 0.0832 - val_accuracy: 1.0000\n",
            "Epoch 135/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0813 - accuracy: 0.9985\n",
            "Epoch 00135: loss did not improve from 0.07771\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0807 - accuracy: 0.9986 - val_loss: 0.0862 - val_accuracy: 0.9919\n",
            "Epoch 136/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0748 - accuracy: 1.0000\n",
            "Epoch 00136: loss improved from 0.07771 to 0.07466, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0747 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 1.0000\n",
            "Epoch 137/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0736 - accuracy: 1.0000\n",
            "Epoch 00137: loss improved from 0.07466 to 0.07334, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0733 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 1.0000\n",
            "Epoch 138/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0704 - accuracy: 1.0000\n",
            "Epoch 00138: loss improved from 0.07334 to 0.07024, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 0.9959\n",
            "Epoch 139/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0685 - accuracy: 1.0000\n",
            "Epoch 00139: loss improved from 0.07024 to 0.06828, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0683 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9959\n",
            "Epoch 140/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0725 - accuracy: 0.9953\n",
            "Epoch 00140: loss did not improve from 0.06828\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0719 - accuracy: 0.9959 - val_loss: 0.0707 - val_accuracy: 1.0000\n",
            "Epoch 141/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0696 - accuracy: 1.0000\n",
            "Epoch 00141: loss did not improve from 0.06828\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
            "Epoch 142/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0673 - accuracy: 0.9985\n",
            "Epoch 00142: loss improved from 0.06828 to 0.06695, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0670 - accuracy: 0.9986 - val_loss: 0.0664 - val_accuracy: 1.0000\n",
            "Epoch 143/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0644 - accuracy: 1.0000\n",
            "Epoch 00143: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0681 - accuracy: 0.9973 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
            "Epoch 144/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0939 - accuracy: 0.9911\n",
            "Epoch 00144: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0933 - accuracy: 0.9918 - val_loss: 0.0890 - val_accuracy: 0.9959\n",
            "Epoch 145/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0749 - accuracy: 0.9970\n",
            "Epoch 00145: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0744 - accuracy: 0.9973 - val_loss: 0.0809 - val_accuracy: 0.9959\n",
            "Epoch 146/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0749 - accuracy: 0.9970\n",
            "Epoch 00146: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0741 - accuracy: 0.9973 - val_loss: 0.0746 - val_accuracy: 1.0000\n",
            "Epoch 147/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0828 - accuracy: 0.9940\n",
            "Epoch 00147: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0824 - accuracy: 0.9946 - val_loss: 0.0960 - val_accuracy: 0.9919\n",
            "Epoch 148/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0993 - accuracy: 0.9836\n",
            "Epoch 00148: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0982 - accuracy: 0.9850 - val_loss: 0.0961 - val_accuracy: 0.9959\n",
            "Epoch 149/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1259 - accuracy: 0.9807\n",
            "Epoch 00149: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1282 - accuracy: 0.9810 - val_loss: 0.1228 - val_accuracy: 0.9878\n",
            "Epoch 150/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1120 - accuracy: 0.9866\n",
            "Epoch 00150: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1166 - accuracy: 0.9837 - val_loss: 0.0907 - val_accuracy: 0.9959\n",
            "Epoch 151/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1181 - accuracy: 0.9851\n",
            "Epoch 00151: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1197 - accuracy: 0.9850 - val_loss: 0.0962 - val_accuracy: 0.9959\n",
            "Epoch 152/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0851 - accuracy: 0.9985\n",
            "Epoch 00152: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0848 - accuracy: 0.9986 - val_loss: 0.0894 - val_accuracy: 0.9959\n",
            "Epoch 153/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0854 - accuracy: 0.9955\n",
            "Epoch 00153: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0849 - accuracy: 0.9959 - val_loss: 0.1072 - val_accuracy: 0.9959\n",
            "Epoch 154/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0834 - accuracy: 0.9970\n",
            "Epoch 00154: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0825 - accuracy: 0.9973 - val_loss: 0.1006 - val_accuracy: 0.9959\n",
            "Epoch 155/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0811 - accuracy: 0.9985\n",
            "Epoch 00155: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0812 - accuracy: 0.9986 - val_loss: 0.0996 - val_accuracy: 0.9919\n",
            "Epoch 156/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1121 - accuracy: 0.9896\n",
            "Epoch 00156: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1111 - accuracy: 0.9905 - val_loss: 0.1470 - val_accuracy: 0.9837\n",
            "Epoch 157/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1114 - accuracy: 0.9836\n",
            "Epoch 00157: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1085 - accuracy: 0.9850 - val_loss: 0.1116 - val_accuracy: 0.9837\n",
            "Epoch 158/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0843 - accuracy: 0.9985\n",
            "Epoch 00158: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0837 - accuracy: 0.9986 - val_loss: 0.1126 - val_accuracy: 0.9878\n",
            "Epoch 159/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0869 - accuracy: 0.9953\n",
            "Epoch 00159: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0900 - accuracy: 0.9946 - val_loss: 0.1545 - val_accuracy: 0.9756\n",
            "Epoch 160/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1025 - accuracy: 0.9896\n",
            "Epoch 00160: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1012 - accuracy: 0.9905 - val_loss: 0.0943 - val_accuracy: 0.9959\n",
            "Epoch 161/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0822 - accuracy: 0.9984\n",
            "Epoch 00161: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0832 - accuracy: 0.9973 - val_loss: 0.0801 - val_accuracy: 1.0000\n",
            "Epoch 162/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0804 - accuracy: 0.9985\n",
            "Epoch 00162: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0808 - accuracy: 0.9986 - val_loss: 0.0851 - val_accuracy: 0.9959\n",
            "Epoch 163/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9946\n",
            "Epoch 00163: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0887 - accuracy: 0.9946 - val_loss: 0.0997 - val_accuracy: 0.9919\n",
            "Epoch 164/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0880 - accuracy: 0.9940\n",
            "Epoch 00164: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0870 - accuracy: 0.9946 - val_loss: 0.0820 - val_accuracy: 0.9959\n",
            "Epoch 165/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0898 - accuracy: 0.9940\n",
            "Epoch 00165: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0892 - accuracy: 0.9946 - val_loss: 0.0915 - val_accuracy: 0.9919\n",
            "Epoch 166/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0823 - accuracy: 0.9970\n",
            "Epoch 00166: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0812 - accuracy: 0.9973 - val_loss: 0.0815 - val_accuracy: 0.9959\n",
            "Epoch 167/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0735 - accuracy: 0.9985\n",
            "Epoch 00167: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0729 - accuracy: 0.9986 - val_loss: 0.0761 - val_accuracy: 0.9959\n",
            "Epoch 168/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0828 - accuracy: 0.9953\n",
            "Epoch 00168: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0810 - accuracy: 0.9959 - val_loss: 0.0816 - val_accuracy: 0.9959\n",
            "Epoch 169/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0876 - accuracy: 0.9940\n",
            "Epoch 00169: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0859 - accuracy: 0.9946 - val_loss: 0.0752 - val_accuracy: 1.0000\n",
            "Epoch 170/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0784 - accuracy: 0.9985\n",
            "Epoch 00170: loss did not improve from 0.06695\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0773 - accuracy: 0.9986 - val_loss: 0.0677 - val_accuracy: 1.0000\n",
            "Epoch 171/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0662 - accuracy: 1.0000\n",
            "Epoch 00171: loss improved from 0.06695 to 0.06596, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0660 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 1.0000\n",
            "Epoch 172/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0674 - accuracy: 0.9985\n",
            "Epoch 00172: loss did not improve from 0.06596\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0670 - accuracy: 0.9986 - val_loss: 0.0827 - val_accuracy: 0.9919\n",
            "Epoch 173/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0640 - accuracy: 1.0000\n",
            "Epoch 00173: loss improved from 0.06596 to 0.06364, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.0658 - val_accuracy: 1.0000\n",
            "Epoch 174/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0607 - accuracy: 1.0000\n",
            "Epoch 00174: loss improved from 0.06364 to 0.06057, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0606 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9959\n",
            "Epoch 175/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0605 - accuracy: 0.9985\n",
            "Epoch 00175: loss improved from 0.06057 to 0.06046, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0605 - accuracy: 0.9986 - val_loss: 0.0644 - val_accuracy: 0.9959\n",
            "Epoch 176/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0575 - accuracy: 1.0000\n",
            "Epoch 00176: loss improved from 0.06046 to 0.05751, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0575 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9959\n",
            "Epoch 177/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0586 - accuracy: 0.9985\n",
            "Epoch 00177: loss did not improve from 0.05751\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0589 - accuracy: 0.9986 - val_loss: 0.0648 - val_accuracy: 0.9959\n",
            "Epoch 178/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0593 - accuracy: 0.9985\n",
            "Epoch 00178: loss did not improve from 0.05751\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0596 - accuracy: 0.9986 - val_loss: 0.0615 - val_accuracy: 1.0000\n",
            "Epoch 179/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0580 - accuracy: 1.0000\n",
            "Epoch 00179: loss did not improve from 0.05751\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0576 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 0.9919\n",
            "Epoch 180/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0568 - accuracy: 1.0000\n",
            "Epoch 00180: loss improved from 0.05751 to 0.05669, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0567 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
            "Epoch 181/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0557 - accuracy: 0.9984\n",
            "Epoch 00181: loss improved from 0.05669 to 0.05548, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 27ms/step - loss: 0.0555 - accuracy: 0.9986 - val_loss: 0.0553 - val_accuracy: 1.0000\n",
            "Epoch 182/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0529 - accuracy: 1.0000\n",
            "Epoch 00182: loss improved from 0.05548 to 0.05266, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0527 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 0.9959\n",
            "Epoch 183/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0513 - accuracy: 1.0000\n",
            "Epoch 00183: loss improved from 0.05266 to 0.05114, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 1.0000\n",
            "Epoch 184/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0515 - accuracy: 0.9985\n",
            "Epoch 00184: loss did not improve from 0.05114\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0512 - accuracy: 0.9986 - val_loss: 0.0562 - val_accuracy: 0.9959\n",
            "Epoch 185/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0499 - accuracy: 1.0000\n",
            "Epoch 00185: loss improved from 0.05114 to 0.04977, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 1.0000\n",
            "Epoch 186/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0493 - accuracy: 1.0000\n",
            "Epoch 00186: loss improved from 0.04977 to 0.04914, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0491 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9959\n",
            "Epoch 187/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0494 - accuracy: 0.9984\n",
            "Epoch 00187: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0497 - accuracy: 0.9986 - val_loss: 0.0664 - val_accuracy: 0.9919\n",
            "Epoch 188/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0505 - accuracy: 0.9985\n",
            "Epoch 00188: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0504 - accuracy: 0.9986 - val_loss: 0.0535 - val_accuracy: 0.9959\n",
            "Epoch 189/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0527 - accuracy: 0.9969\n",
            "Epoch 00189: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.0529 - accuracy: 0.9973 - val_loss: 0.0637 - val_accuracy: 0.9919\n",
            "Epoch 190/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0561 - accuracy: 0.9955\n",
            "Epoch 00190: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0557 - accuracy: 0.9959 - val_loss: 0.0587 - val_accuracy: 0.9959\n",
            "Epoch 191/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0510 - accuracy: 0.9984\n",
            "Epoch 00191: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0503 - accuracy: 0.9986 - val_loss: 0.0688 - val_accuracy: 0.9959\n",
            "Epoch 192/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0764 - accuracy: 0.9896\n",
            "Epoch 00192: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0793 - accuracy: 0.9878 - val_loss: 0.0739 - val_accuracy: 0.9878\n",
            "Epoch 193/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0715 - accuracy: 0.9970\n",
            "Epoch 00193: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0703 - accuracy: 0.9973 - val_loss: 0.0709 - val_accuracy: 0.9919\n",
            "Epoch 194/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0622 - accuracy: 0.9970\n",
            "Epoch 00194: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0645 - accuracy: 0.9959 - val_loss: 0.0653 - val_accuracy: 0.9959\n",
            "Epoch 195/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0630 - accuracy: 0.9955\n",
            "Epoch 00195: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.0633 - accuracy: 0.9959 - val_loss: 0.0766 - val_accuracy: 0.9959\n",
            "Epoch 196/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0691 - accuracy: 0.9937\n",
            "Epoch 00196: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0717 - accuracy: 0.9918 - val_loss: 0.0924 - val_accuracy: 0.9837\n",
            "Epoch 197/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0893 - accuracy: 0.9881\n",
            "Epoch 00197: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0931 - accuracy: 0.9878 - val_loss: 0.0966 - val_accuracy: 0.9878\n",
            "Epoch 198/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0858 - accuracy: 0.9911\n",
            "Epoch 00198: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0995 - accuracy: 0.9878 - val_loss: 0.0802 - val_accuracy: 0.9959\n",
            "Epoch 199/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0972 - accuracy: 0.9881\n",
            "Epoch 00199: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0944 - accuracy: 0.9891 - val_loss: 0.1084 - val_accuracy: 0.9878\n",
            "Epoch 200/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0915 - accuracy: 0.9911\n",
            "Epoch 00200: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0898 - accuracy: 0.9918 - val_loss: 0.0841 - val_accuracy: 0.9959\n",
            "Epoch 201/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0888 - accuracy: 0.9937\n",
            "Epoch 00201: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0857 - accuracy: 0.9946 - val_loss: 0.0791 - val_accuracy: 0.9919\n",
            "Epoch 202/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0671 - accuracy: 1.0000\n",
            "Epoch 00202: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0667 - accuracy: 1.0000 - val_loss: 0.0719 - val_accuracy: 0.9919\n",
            "Epoch 203/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0661 - accuracy: 0.9984\n",
            "Epoch 00203: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0669 - accuracy: 0.9973 - val_loss: 0.0794 - val_accuracy: 0.9959\n",
            "Epoch 204/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0802 - accuracy: 0.9926\n",
            "Epoch 00204: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0809 - accuracy: 0.9918 - val_loss: 0.0748 - val_accuracy: 0.9919\n",
            "Epoch 205/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0810 - accuracy: 0.9926\n",
            "Epoch 00205: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0823 - accuracy: 0.9918 - val_loss: 0.0917 - val_accuracy: 0.9919\n",
            "Epoch 206/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0783 - accuracy: 0.9955\n",
            "Epoch 00206: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0781 - accuracy: 0.9959 - val_loss: 0.1058 - val_accuracy: 0.9919\n",
            "Epoch 207/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0777 - accuracy: 0.9926\n",
            "Epoch 00207: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0765 - accuracy: 0.9932 - val_loss: 0.0744 - val_accuracy: 0.9959\n",
            "Epoch 208/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0712 - accuracy: 0.9953\n",
            "Epoch 00208: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0711 - accuracy: 0.9959 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
            "Epoch 209/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0632 - accuracy: 1.0000\n",
            "Epoch 00209: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0630 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 1.0000\n",
            "Epoch 210/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0665 - accuracy: 0.9970\n",
            "Epoch 00210: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0664 - accuracy: 0.9973 - val_loss: 0.0644 - val_accuracy: 1.0000\n",
            "Epoch 211/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0606 - accuracy: 1.0000\n",
            "Epoch 00211: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0617 - accuracy: 0.9986 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
            "Epoch 212/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0615 - accuracy: 1.0000\n",
            "Epoch 00212: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0613 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9959\n",
            "Epoch 213/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0593 - accuracy: 1.0000\n",
            "Epoch 00213: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.0658 - val_accuracy: 0.9959\n",
            "Epoch 214/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0563 - accuracy: 1.0000\n",
            "Epoch 00214: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0561 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
            "Epoch 215/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0559 - accuracy: 1.0000\n",
            "Epoch 00215: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9959\n",
            "Epoch 216/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0593 - accuracy: 0.9970\n",
            "Epoch 00216: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0588 - accuracy: 0.9973 - val_loss: 0.0811 - val_accuracy: 0.9919\n",
            "Epoch 217/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0650 - accuracy: 0.9969\n",
            "Epoch 00217: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0643 - accuracy: 0.9973 - val_loss: 0.0599 - val_accuracy: 0.9959\n",
            "Epoch 218/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0712 - accuracy: 0.9911\n",
            "Epoch 00218: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0705 - accuracy: 0.9918 - val_loss: 0.0691 - val_accuracy: 1.0000\n",
            "Epoch 219/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0685 - accuracy: 0.9953\n",
            "Epoch 00219: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0673 - accuracy: 0.9959 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
            "Epoch 220/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0610 - accuracy: 0.9985\n",
            "Epoch 00220: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0621 - accuracy: 0.9973 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
            "Epoch 221/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0911 - accuracy: 0.9866\n",
            "Epoch 00221: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0896 - accuracy: 0.9878 - val_loss: 0.0850 - val_accuracy: 0.9919\n",
            "Epoch 222/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0764 - accuracy: 0.9937\n",
            "Epoch 00222: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0766 - accuracy: 0.9932 - val_loss: 0.0703 - val_accuracy: 0.9959\n",
            "Epoch 223/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0606 - accuracy: 1.0000\n",
            "Epoch 00223: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0616 - accuracy: 0.9986 - val_loss: 0.0655 - val_accuracy: 0.9959\n",
            "Epoch 224/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0676 - accuracy: 0.9970\n",
            "Epoch 00224: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0669 - accuracy: 0.9973 - val_loss: 0.0927 - val_accuracy: 0.9919\n",
            "Epoch 225/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0973 - accuracy: 0.9866\n",
            "Epoch 00225: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0952 - accuracy: 0.9878 - val_loss: 0.0864 - val_accuracy: 0.9959\n",
            "Epoch 226/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0704 - accuracy: 0.9970\n",
            "Epoch 00226: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0700 - accuracy: 0.9973 - val_loss: 0.0685 - val_accuracy: 0.9959\n",
            "Epoch 227/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0602 - accuracy: 1.0000\n",
            "Epoch 00227: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0607 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 1.0000\n",
            "Epoch 228/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0585 - accuracy: 0.9985\n",
            "Epoch 00228: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0583 - accuracy: 0.9986 - val_loss: 0.0645 - val_accuracy: 0.9959\n",
            "Epoch 229/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0596 - accuracy: 0.9984\n",
            "Epoch 00229: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0594 - accuracy: 0.9986 - val_loss: 0.0589 - val_accuracy: 1.0000\n",
            "Epoch 230/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0577 - accuracy: 0.9985\n",
            "Epoch 00230: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0578 - accuracy: 0.9986 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
            "Epoch 231/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0766 - accuracy: 0.9940\n",
            "Epoch 00231: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0842 - accuracy: 0.9905 - val_loss: 0.0791 - val_accuracy: 0.9959\n",
            "Epoch 232/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1016 - accuracy: 0.9866\n",
            "Epoch 00232: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0988 - accuracy: 0.9878 - val_loss: 0.0928 - val_accuracy: 0.9919\n",
            "Epoch 233/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0826 - accuracy: 0.9940\n",
            "Epoch 00233: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0805 - accuracy: 0.9946 - val_loss: 0.0899 - val_accuracy: 0.9878\n",
            "Epoch 234/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0673 - accuracy: 0.9955\n",
            "Epoch 00234: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0673 - accuracy: 0.9959 - val_loss: 0.0751 - val_accuracy: 0.9959\n",
            "Epoch 235/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0592 - accuracy: 0.9985\n",
            "Epoch 00235: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0592 - accuracy: 0.9986 - val_loss: 0.0688 - val_accuracy: 0.9919\n",
            "Epoch 236/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0573 - accuracy: 1.0000\n",
            "Epoch 00236: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0571 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 0.9959\n",
            "Epoch 237/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0549 - accuracy: 1.0000\n",
            "Epoch 00237: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0549 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9959\n",
            "Epoch 238/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0546 - accuracy: 1.0000\n",
            "Epoch 00238: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9959\n",
            "Epoch 239/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0545 - accuracy: 0.9985\n",
            "Epoch 00239: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0542 - accuracy: 0.9986 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
            "Epoch 240/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0693 - accuracy: 0.9940\n",
            "Epoch 00240: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0724 - accuracy: 0.9918 - val_loss: 0.0628 - val_accuracy: 1.0000\n",
            "Epoch 241/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0721 - accuracy: 0.9926\n",
            "Epoch 00241: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0716 - accuracy: 0.9932 - val_loss: 0.0719 - val_accuracy: 0.9919\n",
            "Epoch 242/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0629 - accuracy: 0.9985\n",
            "Epoch 00242: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0663 - accuracy: 0.9973 - val_loss: 0.0582 - val_accuracy: 1.0000\n",
            "Epoch 243/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0620 - accuracy: 0.9970\n",
            "Epoch 00243: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0616 - accuracy: 0.9973 - val_loss: 0.0647 - val_accuracy: 0.9959\n",
            "Epoch 244/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0623 - accuracy: 0.9955\n",
            "Epoch 00244: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0614 - accuracy: 0.9959 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
            "Epoch 245/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1191 - accuracy: 0.9792\n",
            "Epoch 00245: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1164 - accuracy: 0.9796 - val_loss: 0.0995 - val_accuracy: 0.9878\n",
            "Epoch 246/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0861 - accuracy: 0.9911\n",
            "Epoch 00246: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.0841 - accuracy: 0.9918 - val_loss: 0.0908 - val_accuracy: 0.9837\n",
            "Epoch 247/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0831 - accuracy: 0.9906\n",
            "Epoch 00247: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0855 - accuracy: 0.9905 - val_loss: 0.0755 - val_accuracy: 0.9959\n",
            "Epoch 248/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0729 - accuracy: 0.9970\n",
            "Epoch 00248: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0731 - accuracy: 0.9959 - val_loss: 0.0667 - val_accuracy: 0.9959\n",
            "Epoch 249/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0718 - accuracy: 0.9926\n",
            "Epoch 00249: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0710 - accuracy: 0.9932 - val_loss: 0.0642 - val_accuracy: 0.9959\n",
            "Epoch 250/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0714 - accuracy: 0.9955\n",
            "Epoch 00250: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0749 - accuracy: 0.9932 - val_loss: 0.0636 - val_accuracy: 1.0000\n",
            "Epoch 251/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0721 - accuracy: 0.9970\n",
            "Epoch 00251: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0712 - accuracy: 0.9973 - val_loss: 0.0738 - val_accuracy: 0.9959\n",
            "Epoch 252/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0658 - accuracy: 0.9970\n",
            "Epoch 00252: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0653 - accuracy: 0.9973 - val_loss: 0.0833 - val_accuracy: 0.9919\n",
            "Epoch 253/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0665 - accuracy: 0.9970\n",
            "Epoch 00253: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0658 - accuracy: 0.9973 - val_loss: 0.0646 - val_accuracy: 0.9959\n",
            "Epoch 254/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0631 - accuracy: 0.9985\n",
            "Epoch 00254: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0630 - accuracy: 0.9986 - val_loss: 0.0776 - val_accuracy: 0.9919\n",
            "Epoch 255/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0587 - accuracy: 1.0000\n",
            "Epoch 00255: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0585 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
            "Epoch 256/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0596 - accuracy: 1.0000\n",
            "Epoch 00256: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0594 - accuracy: 1.0000 - val_loss: 0.0602 - val_accuracy: 0.9959\n",
            "Epoch 257/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0557 - accuracy: 1.0000\n",
            "Epoch 00257: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0556 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 1.0000\n",
            "Epoch 258/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0539 - accuracy: 1.0000\n",
            "Epoch 00258: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0537 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
            "Epoch 259/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0519 - accuracy: 1.0000\n",
            "Epoch 00259: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0518 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
            "Epoch 260/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0510 - accuracy: 1.0000\n",
            "Epoch 00260: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
            "Epoch 261/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0493 - accuracy: 1.0000\n",
            "Epoch 00261: loss did not improve from 0.04914\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 1.0000\n",
            "Epoch 262/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0488 - accuracy: 1.0000\n",
            "Epoch 00262: loss improved from 0.04914 to 0.04874, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
            "Epoch 263/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0482 - accuracy: 1.0000\n",
            "Epoch 00263: loss improved from 0.04874 to 0.04806, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 1.0000\n",
            "Epoch 264/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0466 - accuracy: 1.0000\n",
            "Epoch 00264: loss improved from 0.04806 to 0.04648, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 1.0000\n",
            "Epoch 265/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9986\n",
            "Epoch 00265: loss did not improve from 0.04648\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0474 - accuracy: 0.9986 - val_loss: 0.0524 - val_accuracy: 0.9959\n",
            "Epoch 266/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0458 - accuracy: 1.0000\n",
            "Epoch 00266: loss improved from 0.04648 to 0.04584, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0458 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
            "Epoch 267/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0473 - accuracy: 0.9985\n",
            "Epoch 00267: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0470 - accuracy: 0.9986 - val_loss: 0.0685 - val_accuracy: 0.9919\n",
            "Epoch 268/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0600 - accuracy: 0.9955\n",
            "Epoch 00268: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0586 - accuracy: 0.9959 - val_loss: 0.0560 - val_accuracy: 1.0000\n",
            "Epoch 269/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0587 - accuracy: 0.9953\n",
            "Epoch 00269: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0573 - accuracy: 0.9959 - val_loss: 0.0947 - val_accuracy: 0.9837\n",
            "Epoch 270/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0816 - accuracy: 0.9922\n",
            "Epoch 00270: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0796 - accuracy: 0.9932 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
            "Epoch 271/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0804 - accuracy: 0.9881\n",
            "Epoch 00271: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0807 - accuracy: 0.9891 - val_loss: 0.0825 - val_accuracy: 0.9919\n",
            "Epoch 272/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0823 - accuracy: 0.9866\n",
            "Epoch 00272: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0812 - accuracy: 0.9864 - val_loss: 0.0664 - val_accuracy: 0.9959\n",
            "Epoch 273/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0621 - accuracy: 0.9985\n",
            "Epoch 00273: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0614 - accuracy: 0.9986 - val_loss: 0.0800 - val_accuracy: 0.9919\n",
            "Epoch 274/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0674 - accuracy: 0.9955\n",
            "Epoch 00274: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0670 - accuracy: 0.9959 - val_loss: 0.0748 - val_accuracy: 0.9919\n",
            "Epoch 275/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9866\n",
            "Epoch 00275: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0846 - accuracy: 0.9864 - val_loss: 0.0843 - val_accuracy: 0.9919\n",
            "Epoch 276/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0716 - accuracy: 0.9940\n",
            "Epoch 00276: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0742 - accuracy: 0.9932 - val_loss: 0.0742 - val_accuracy: 0.9959\n",
            "Epoch 277/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0826 - accuracy: 0.9906\n",
            "Epoch 00277: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0800 - accuracy: 0.9918 - val_loss: 0.0638 - val_accuracy: 0.9959\n",
            "Epoch 278/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0686 - accuracy: 0.9969\n",
            "Epoch 00278: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0677 - accuracy: 0.9973 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
            "Epoch 279/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0565 - accuracy: 1.0000\n",
            "Epoch 00279: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 1.0000\n",
            "Epoch 280/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0535 - accuracy: 1.0000\n",
            "Epoch 00280: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0534 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
            "Epoch 281/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0525 - accuracy: 1.0000\n",
            "Epoch 00281: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 1.0000\n",
            "Epoch 282/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0510 - accuracy: 1.0000\n",
            "Epoch 00282: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 1.0000\n",
            "Epoch 283/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0515 - accuracy: 1.0000\n",
            "Epoch 00283: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0514 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 1.0000\n",
            "Epoch 284/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0501 - accuracy: 1.0000\n",
            "Epoch 00284: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9959\n",
            "Epoch 285/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0494 - accuracy: 1.0000\n",
            "Epoch 00285: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
            "Epoch 286/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0484 - accuracy: 1.0000\n",
            "Epoch 00286: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0483 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
            "Epoch 287/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0461 - accuracy: 1.0000\n",
            "Epoch 00287: loss did not improve from 0.04584\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "Epoch 288/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0453 - accuracy: 1.0000\n",
            "Epoch 00288: loss improved from 0.04584 to 0.04524, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
            "Epoch 289/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0447 - accuracy: 1.0000\n",
            "Epoch 00289: loss improved from 0.04524 to 0.04466, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
            "Epoch 290/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0442 - accuracy: 1.0000\n",
            "Epoch 00290: loss improved from 0.04466 to 0.04401, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
            "Epoch 291/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0439 - accuracy: 1.0000\n",
            "Epoch 00291: loss improved from 0.04401 to 0.04381, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "Epoch 292/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0469 - accuracy: 0.9969\n",
            "Epoch 00292: loss did not improve from 0.04381\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0462 - accuracy: 0.9973 - val_loss: 0.0566 - val_accuracy: 0.9959\n",
            "Epoch 293/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0448 - accuracy: 1.0000\n",
            "Epoch 00293: loss did not improve from 0.04381\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0446 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9959\n",
            "Epoch 294/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0433 - accuracy: 1.0000\n",
            "Epoch 00294: loss improved from 0.04381 to 0.04311, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.9959\n",
            "Epoch 295/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0475 - accuracy: 0.9970\n",
            "Epoch 00295: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0470 - accuracy: 0.9973 - val_loss: 0.0645 - val_accuracy: 0.9919\n",
            "Epoch 296/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0609 - accuracy: 0.9940\n",
            "Epoch 00296: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0597 - accuracy: 0.9946 - val_loss: 0.0647 - val_accuracy: 0.9919\n",
            "Epoch 297/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9864\n",
            "Epoch 00297: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0719 - accuracy: 0.9864 - val_loss: 0.0605 - val_accuracy: 1.0000\n",
            "Epoch 298/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0832 - accuracy: 0.9896\n",
            "Epoch 00298: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0816 - accuracy: 0.9905 - val_loss: 0.0694 - val_accuracy: 0.9959\n",
            "Epoch 299/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0622 - accuracy: 0.9940\n",
            "Epoch 00299: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0614 - accuracy: 0.9946 - val_loss: 0.0671 - val_accuracy: 0.9959\n",
            "Epoch 300/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0519 - accuracy: 0.9985\n",
            "Epoch 00300: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0522 - accuracy: 0.9986 - val_loss: 0.0527 - val_accuracy: 0.9959\n",
            "Epoch 301/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0532 - accuracy: 0.9969\n",
            "Epoch 00301: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0532 - accuracy: 0.9973 - val_loss: 0.0596 - val_accuracy: 0.9959\n",
            "Epoch 302/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0492 - accuracy: 1.0000\n",
            "Epoch 00302: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0529 - accuracy: 0.9986 - val_loss: 0.0514 - val_accuracy: 1.0000\n",
            "Epoch 303/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0577 - accuracy: 0.9969\n",
            "Epoch 00303: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0566 - accuracy: 0.9973 - val_loss: 0.0537 - val_accuracy: 1.0000\n",
            "Epoch 304/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0549 - accuracy: 0.9970\n",
            "Epoch 00304: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0541 - accuracy: 0.9973 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
            "Epoch 305/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0509 - accuracy: 1.0000\n",
            "Epoch 00305: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9919\n",
            "Epoch 306/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0511 - accuracy: 0.9969\n",
            "Epoch 00306: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0503 - accuracy: 0.9973 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
            "Epoch 307/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0463 - accuracy: 1.0000\n",
            "Epoch 00307: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0461 - accuracy: 1.0000 - val_loss: 0.0468 - val_accuracy: 1.0000\n",
            "Epoch 308/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0449 - accuracy: 1.0000\n",
            "Epoch 00308: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
            "Epoch 309/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0432 - accuracy: 1.0000\n",
            "Epoch 00309: loss did not improve from 0.04311\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.0433 - val_accuracy: 1.0000\n",
            "Epoch 310/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0425 - accuracy: 1.0000\n",
            "Epoch 00310: loss improved from 0.04311 to 0.04247, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 311/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0443 - accuracy: 0.9985\n",
            "Epoch 00311: loss did not improve from 0.04247\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0439 - accuracy: 0.9986 - val_loss: 0.0506 - val_accuracy: 0.9959\n",
            "Epoch 312/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0812 - accuracy: 0.9911\n",
            "Epoch 00312: loss did not improve from 0.04247\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0794 - accuracy: 0.9918 - val_loss: 0.1002 - val_accuracy: 0.9715\n",
            "Epoch 313/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0747 - accuracy: 0.9896\n",
            "Epoch 00313: loss did not improve from 0.04247\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0724 - accuracy: 0.9905 - val_loss: 0.0692 - val_accuracy: 0.9919\n",
            "Epoch 314/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0547 - accuracy: 0.9955\n",
            "Epoch 00314: loss did not improve from 0.04247\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0574 - accuracy: 0.9946 - val_loss: 0.0543 - val_accuracy: 1.0000\n",
            "Epoch 315/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0513 - accuracy: 1.0000\n",
            "Epoch 00315: loss did not improve from 0.04247\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
            "Epoch 316/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0465 - accuracy: 1.0000\n",
            "Epoch 00316: loss did not improve from 0.04247\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9959\n",
            "Epoch 317/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0440 - accuracy: 1.0000\n",
            "Epoch 00317: loss did not improve from 0.04247\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
            "Epoch 318/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n",
            "Epoch 00318: loss did not improve from 0.04247\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
            "Epoch 319/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0420 - accuracy: 1.0000\n",
            "Epoch 00319: loss improved from 0.04247 to 0.04196, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 1.0000\n",
            "Epoch 320/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0419 - accuracy: 1.0000\n",
            "Epoch 00320: loss improved from 0.04196 to 0.04190, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 321/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0420 - accuracy: 0.9985\n",
            "Epoch 00321: loss did not improve from 0.04190\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0420 - accuracy: 0.9986 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
            "Epoch 322/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0404 - accuracy: 1.0000\n",
            "Epoch 00322: loss improved from 0.04190 to 0.04027, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
            "Epoch 323/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 00323: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
            "Epoch 324/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0438 - accuracy: 0.9969\n",
            "Epoch 00324: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0433 - accuracy: 0.9973 - val_loss: 0.0456 - val_accuracy: 0.9959\n",
            "Epoch 325/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0425 - accuracy: 0.9970\n",
            "Epoch 00325: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0433 - accuracy: 0.9973 - val_loss: 0.0459 - val_accuracy: 0.9959\n",
            "Epoch 326/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0534 - accuracy: 0.9955\n",
            "Epoch 00326: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0524 - accuracy: 0.9959 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
            "Epoch 327/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0621 - accuracy: 0.9955\n",
            "Epoch 00327: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0603 - accuracy: 0.9959 - val_loss: 0.0470 - val_accuracy: 0.9959\n",
            "Epoch 328/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0575 - accuracy: 0.9970\n",
            "Epoch 00328: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0565 - accuracy: 0.9973 - val_loss: 0.0758 - val_accuracy: 0.9878\n",
            "Epoch 329/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0863 - accuracy: 0.9866\n",
            "Epoch 00329: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0856 - accuracy: 0.9864 - val_loss: 0.0606 - val_accuracy: 0.9959\n",
            "Epoch 330/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0548 - accuracy: 0.9955\n",
            "Epoch 00330: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0542 - accuracy: 0.9959 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
            "Epoch 331/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0579 - accuracy: 0.9953\n",
            "Epoch 00331: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0569 - accuracy: 0.9959 - val_loss: 0.0481 - val_accuracy: 1.0000\n",
            "Epoch 332/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0713 - accuracy: 0.9896\n",
            "Epoch 00332: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0721 - accuracy: 0.9891 - val_loss: 0.1307 - val_accuracy: 0.9756\n",
            "Epoch 333/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0820 - accuracy: 0.9906\n",
            "Epoch 00333: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0868 - accuracy: 0.9878 - val_loss: 0.1030 - val_accuracy: 0.9878\n",
            "Epoch 334/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1097 - accuracy: 0.9762\n",
            "Epoch 00334: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1153 - accuracy: 0.9728 - val_loss: 0.0628 - val_accuracy: 1.0000\n",
            "Epoch 335/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0721 - accuracy: 0.9940\n",
            "Epoch 00335: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0703 - accuracy: 0.9946 - val_loss: 0.0660 - val_accuracy: 0.9959\n",
            "Epoch 336/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0541 - accuracy: 1.0000\n",
            "Epoch 00336: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.0575 - val_accuracy: 0.9959\n",
            "Epoch 337/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0520 - accuracy: 1.0000\n",
            "Epoch 00337: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0533 - accuracy: 0.9986 - val_loss: 0.0578 - val_accuracy: 1.0000\n",
            "Epoch 338/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0588 - accuracy: 0.9969\n",
            "Epoch 00338: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0576 - accuracy: 0.9973 - val_loss: 0.0553 - val_accuracy: 0.9959\n",
            "Epoch 339/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0572 - accuracy: 0.9969\n",
            "Epoch 00339: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0564 - accuracy: 0.9973 - val_loss: 0.0509 - val_accuracy: 1.0000\n",
            "Epoch 340/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0512 - accuracy: 1.0000\n",
            "Epoch 00340: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0515 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 0.9919\n",
            "Epoch 341/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0515 - accuracy: 0.9985\n",
            "Epoch 00341: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0521 - accuracy: 0.9986 - val_loss: 0.0609 - val_accuracy: 0.9959\n",
            "Epoch 342/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0476 - accuracy: 1.0000\n",
            "Epoch 00342: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0487 - accuracy: 0.9986 - val_loss: 0.0711 - val_accuracy: 0.9919\n",
            "Epoch 343/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0491 - accuracy: 0.9984\n",
            "Epoch 00343: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0488 - accuracy: 0.9986 - val_loss: 0.0687 - val_accuracy: 0.9878\n",
            "Epoch 344/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0488 - accuracy: 1.0000\n",
            "Epoch 00344: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9959\n",
            "Epoch 345/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0466 - accuracy: 1.0000\n",
            "Epoch 00345: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0463 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9959\n",
            "Epoch 346/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0447 - accuracy: 1.0000\n",
            "Epoch 00346: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 0.9959\n",
            "Epoch 347/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0440 - accuracy: 1.0000\n",
            "Epoch 00347: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9959\n",
            "Epoch 348/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0444 - accuracy: 0.9985\n",
            "Epoch 00348: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0443 - accuracy: 0.9986 - val_loss: 0.0517 - val_accuracy: 0.9959\n",
            "Epoch 349/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0426 - accuracy: 1.0000\n",
            "Epoch 00349: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 0.9959\n",
            "Epoch 350/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0482 - accuracy: 0.9985\n",
            "Epoch 00350: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0478 - accuracy: 0.9986 - val_loss: 0.0563 - val_accuracy: 0.9919\n",
            "Epoch 351/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0484 - accuracy: 0.9955\n",
            "Epoch 00351: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0483 - accuracy: 0.9959 - val_loss: 0.0674 - val_accuracy: 0.9919\n",
            "Epoch 352/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0440 - accuracy: 1.0000\n",
            "Epoch 00352: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.0483 - val_accuracy: 0.9959\n",
            "Epoch 353/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0413 - accuracy: 1.0000\n",
            "Epoch 00353: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
            "Epoch 354/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0408 - accuracy: 1.0000\n",
            "Epoch 00354: loss did not improve from 0.04027\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 1.0000\n",
            "Epoch 355/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0392 - accuracy: 1.0000\n",
            "Epoch 00355: loss improved from 0.04027 to 0.03911, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 356/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 00356: loss improved from 0.03911 to 0.03852, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.9919\n",
            "Epoch 357/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000\n",
            "Epoch 00357: loss did not improve from 0.03852\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 358/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0374 - accuracy: 1.0000\n",
            "Epoch 00358: loss improved from 0.03852 to 0.03729, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9959\n",
            "Epoch 359/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0444 - accuracy: 0.9970\n",
            "Epoch 00359: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0438 - accuracy: 0.9973 - val_loss: 0.0541 - val_accuracy: 0.9919\n",
            "Epoch 360/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0535 - accuracy: 0.9940\n",
            "Epoch 00360: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0534 - accuracy: 0.9946 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
            "Epoch 361/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0661 - accuracy: 0.9926\n",
            "Epoch 00361: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0683 - accuracy: 0.9905 - val_loss: 0.0562 - val_accuracy: 0.9959\n",
            "Epoch 362/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0694 - accuracy: 0.9937\n",
            "Epoch 00362: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0668 - accuracy: 0.9946 - val_loss: 0.0657 - val_accuracy: 0.9959\n",
            "Epoch 363/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0484 - accuracy: 0.9985\n",
            "Epoch 00363: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0500 - accuracy: 0.9973 - val_loss: 0.0434 - val_accuracy: 1.0000\n",
            "Epoch 364/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0569 - accuracy: 0.9926\n",
            "Epoch 00364: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0557 - accuracy: 0.9932 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
            "Epoch 365/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0575 - accuracy: 0.9955\n",
            "Epoch 00365: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0584 - accuracy: 0.9959 - val_loss: 0.0667 - val_accuracy: 0.9959\n",
            "Epoch 366/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0533 - accuracy: 0.9970\n",
            "Epoch 00366: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0528 - accuracy: 0.9973 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
            "Epoch 367/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0605 - accuracy: 0.9940\n",
            "Epoch 00367: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0607 - accuracy: 0.9932 - val_loss: 0.0487 - val_accuracy: 0.9959\n",
            "Epoch 368/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0717 - accuracy: 0.9891\n",
            "Epoch 00368: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0709 - accuracy: 0.9891 - val_loss: 0.0714 - val_accuracy: 0.9878\n",
            "Epoch 369/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0587 - accuracy: 0.9955\n",
            "Epoch 00369: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0581 - accuracy: 0.9959 - val_loss: 0.0540 - val_accuracy: 1.0000\n",
            "Epoch 370/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0571 - accuracy: 0.9985\n",
            "Epoch 00370: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0563 - accuracy: 0.9986 - val_loss: 0.0545 - val_accuracy: 0.9959\n",
            "Epoch 371/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0535 - accuracy: 0.9970\n",
            "Epoch 00371: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0531 - accuracy: 0.9973 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
            "Epoch 372/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0628 - accuracy: 0.9940\n",
            "Epoch 00372: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0617 - accuracy: 0.9946 - val_loss: 0.0634 - val_accuracy: 0.9959\n",
            "Epoch 373/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0539 - accuracy: 0.9985\n",
            "Epoch 00373: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0533 - accuracy: 0.9986 - val_loss: 0.0563 - val_accuracy: 0.9919\n",
            "Epoch 374/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0475 - accuracy: 1.0000\n",
            "Epoch 00374: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 1.0000\n",
            "Epoch 375/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0450 - accuracy: 1.0000\n",
            "Epoch 00375: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
            "Epoch 376/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0455 - accuracy: 1.0000\n",
            "Epoch 00376: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0468 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "Epoch 377/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0433 - accuracy: 1.0000\n",
            "Epoch 00377: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
            "Epoch 378/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0421 - accuracy: 1.0000\n",
            "Epoch 00378: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
            "Epoch 379/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0427 - accuracy: 1.0000\n",
            "Epoch 00379: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 1.0000\n",
            "Epoch 380/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0411 - accuracy: 1.0000\n",
            "Epoch 00380: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
            "Epoch 381/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0395 - accuracy: 1.0000\n",
            "Epoch 00381: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 0.9959\n",
            "Epoch 382/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 00382: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9959\n",
            "Epoch 383/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000\n",
            "Epoch 00383: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
            "Epoch 384/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 00384: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
            "Epoch 385/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0488 - accuracy: 0.9955\n",
            "Epoch 00385: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0486 - accuracy: 0.9959 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
            "Epoch 386/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0441 - accuracy: 0.9970\n",
            "Epoch 00386: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0454 - accuracy: 0.9959 - val_loss: 0.0431 - val_accuracy: 1.0000\n",
            "Epoch 387/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0411 - accuracy: 1.0000\n",
            "Epoch 00387: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.0418 - val_accuracy: 1.0000\n",
            "Epoch 388/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
            "Epoch 00388: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
            "Epoch 389/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0386 - accuracy: 1.0000\n",
            "Epoch 00389: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 0.9959\n",
            "Epoch 390/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0576 - accuracy: 0.9937\n",
            "Epoch 00390: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0556 - accuracy: 0.9946 - val_loss: 0.0482 - val_accuracy: 0.9959\n",
            "Epoch 391/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0595 - accuracy: 0.9940\n",
            "Epoch 00391: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.0636 - accuracy: 0.9932 - val_loss: 0.0590 - val_accuracy: 0.9959\n",
            "Epoch 392/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0672 - accuracy: 0.9866\n",
            "Epoch 00392: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0649 - accuracy: 0.9878 - val_loss: 0.0521 - val_accuracy: 0.9959\n",
            "Epoch 393/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0644 - accuracy: 0.9881\n",
            "Epoch 00393: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0631 - accuracy: 0.9891 - val_loss: 0.0607 - val_accuracy: 0.9959\n",
            "Epoch 394/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0838 - accuracy: 0.9821\n",
            "Epoch 00394: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0834 - accuracy: 0.9823 - val_loss: 0.0654 - val_accuracy: 0.9959\n",
            "Epoch 395/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0697 - accuracy: 0.9911\n",
            "Epoch 00395: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0679 - accuracy: 0.9918 - val_loss: 0.0728 - val_accuracy: 0.9919\n",
            "Epoch 396/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0501 - accuracy: 1.0000\n",
            "Epoch 00396: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0497 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9959\n",
            "Epoch 397/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0490 - accuracy: 0.9970\n",
            "Epoch 00397: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0486 - accuracy: 0.9973 - val_loss: 0.0672 - val_accuracy: 0.9959\n",
            "Epoch 398/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0555 - accuracy: 0.9940\n",
            "Epoch 00398: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0545 - accuracy: 0.9946 - val_loss: 0.0647 - val_accuracy: 0.9959\n",
            "Epoch 399/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0524 - accuracy: 0.9953\n",
            "Epoch 00399: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0520 - accuracy: 0.9959 - val_loss: 0.0598 - val_accuracy: 0.9959\n",
            "Epoch 400/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0481 - accuracy: 0.9985\n",
            "Epoch 00400: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0478 - accuracy: 0.9986 - val_loss: 0.0615 - val_accuracy: 0.9959\n",
            "Epoch 401/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0446 - accuracy: 1.0000\n",
            "Epoch 00401: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0444 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "Epoch 402/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0433 - accuracy: 1.0000\n",
            "Epoch 00402: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0447 - accuracy: 0.9986 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
            "Epoch 403/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n",
            "Epoch 00403: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.0468 - val_accuracy: 0.9959\n",
            "Epoch 404/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0417 - accuracy: 1.0000\n",
            "Epoch 00404: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 1.0000\n",
            "Epoch 405/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0439 - accuracy: 0.9970\n",
            "Epoch 00405: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0436 - accuracy: 0.9973 - val_loss: 0.0443 - val_accuracy: 0.9959\n",
            "Epoch 406/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0411 - accuracy: 1.0000\n",
            "Epoch 00406: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
            "Epoch 407/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n",
            "Epoch 00407: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
            "Epoch 408/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n",
            "Epoch 00408: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9959\n",
            "Epoch 409/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0421 - accuracy: 0.9985\n",
            "Epoch 00409: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0417 - accuracy: 0.9986 - val_loss: 0.0451 - val_accuracy: 0.9959\n",
            "Epoch 410/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0416 - accuracy: 0.9985\n",
            "Epoch 00410: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0418 - accuracy: 0.9986 - val_loss: 0.0455 - val_accuracy: 0.9959\n",
            "Epoch 411/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0551 - accuracy: 0.9940\n",
            "Epoch 00411: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0562 - accuracy: 0.9932 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
            "Epoch 412/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0458 - accuracy: 1.0000\n",
            "Epoch 00412: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.0418 - val_accuracy: 1.0000\n",
            "Epoch 413/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0554 - accuracy: 0.9926\n",
            "Epoch 00413: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0549 - accuracy: 0.9932 - val_loss: 0.1285 - val_accuracy: 0.9715\n",
            "Epoch 414/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1134 - accuracy: 0.9777\n",
            "Epoch 00414: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1175 - accuracy: 0.9755 - val_loss: 0.1003 - val_accuracy: 0.9756\n",
            "Epoch 415/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0839 - accuracy: 0.9896\n",
            "Epoch 00415: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0817 - accuracy: 0.9905 - val_loss: 0.0667 - val_accuracy: 0.9919\n",
            "Epoch 416/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0560 - accuracy: 0.9955\n",
            "Epoch 00416: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0552 - accuracy: 0.9959 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
            "Epoch 417/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0634 - accuracy: 0.9955\n",
            "Epoch 00417: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0624 - accuracy: 0.9959 - val_loss: 0.0769 - val_accuracy: 0.9919\n",
            "Epoch 418/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0563 - accuracy: 0.9984\n",
            "Epoch 00418: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0562 - accuracy: 0.9986 - val_loss: 0.0647 - val_accuracy: 0.9919\n",
            "Epoch 419/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0552 - accuracy: 0.9970\n",
            "Epoch 00419: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0569 - accuracy: 0.9959 - val_loss: 0.0601 - val_accuracy: 0.9959\n",
            "Epoch 420/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0546 - accuracy: 0.9970\n",
            "Epoch 00420: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0546 - accuracy: 0.9973 - val_loss: 0.0497 - val_accuracy: 1.0000\n",
            "Epoch 421/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0502 - accuracy: 0.9984\n",
            "Epoch 00421: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0515 - accuracy: 0.9973 - val_loss: 0.0511 - val_accuracy: 0.9959\n",
            "Epoch 422/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0453 - accuracy: 1.0000\n",
            "Epoch 00422: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 1.0000\n",
            "Epoch 423/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0458 - accuracy: 0.9985\n",
            "Epoch 00423: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0457 - accuracy: 0.9986 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
            "Epoch 424/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0441 - accuracy: 1.0000\n",
            "Epoch 00424: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
            "Epoch 425/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0438 - accuracy: 1.0000\n",
            "Epoch 00425: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 426/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0436 - accuracy: 1.0000\n",
            "Epoch 00426: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 427/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0452 - accuracy: 0.9984\n",
            "Epoch 00427: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0449 - accuracy: 0.9986 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
            "Epoch 428/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0459 - accuracy: 0.9970\n",
            "Epoch 00428: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0460 - accuracy: 0.9973 - val_loss: 0.0471 - val_accuracy: 0.9959\n",
            "Epoch 429/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0492 - accuracy: 0.9970\n",
            "Epoch 00429: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0486 - accuracy: 0.9973 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
            "Epoch 430/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0428 - accuracy: 1.0000\n",
            "Epoch 00430: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "Epoch 431/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0403 - accuracy: 1.0000\n",
            "Epoch 00431: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
            "Epoch 432/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0408 - accuracy: 1.0000\n",
            "Epoch 00432: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9959\n",
            "Epoch 433/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0650 - accuracy: 0.9926\n",
            "Epoch 00433: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0641 - accuracy: 0.9932 - val_loss: 0.0570 - val_accuracy: 0.9919\n",
            "Epoch 434/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0547 - accuracy: 0.9955\n",
            "Epoch 00434: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0539 - accuracy: 0.9959 - val_loss: 0.0577 - val_accuracy: 0.9959\n",
            "Epoch 435/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0578 - accuracy: 0.9926\n",
            "Epoch 00435: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0578 - accuracy: 0.9932 - val_loss: 0.0431 - val_accuracy: 1.0000\n",
            "Epoch 436/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0516 - accuracy: 0.9955\n",
            "Epoch 00436: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0551 - accuracy: 0.9946 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
            "Epoch 437/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0496 - accuracy: 0.9970\n",
            "Epoch 00437: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0497 - accuracy: 0.9973 - val_loss: 0.0463 - val_accuracy: 1.0000\n",
            "Epoch 438/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0448 - accuracy: 0.9985\n",
            "Epoch 00438: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0446 - accuracy: 0.9986 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
            "Epoch 439/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0426 - accuracy: 1.0000\n",
            "Epoch 00439: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0424 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 1.0000\n",
            "Epoch 440/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0416 - accuracy: 1.0000\n",
            "Epoch 00440: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
            "Epoch 441/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
            "Epoch 00441: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0417 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 1.0000\n",
            "Epoch 442/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0401 - accuracy: 1.0000\n",
            "Epoch 00442: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 443/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 00443: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
            "Epoch 444/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n",
            "Epoch 00444: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 445/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
            "Epoch 00445: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 446/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0387 - accuracy: 0.9984\n",
            "Epoch 00446: loss did not improve from 0.03729\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0383 - accuracy: 0.9986 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 447/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
            "Epoch 00447: loss improved from 0.03729 to 0.03699, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "Epoch 448/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 00448: loss improved from 0.03699 to 0.03634, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
            "Epoch 449/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 00449: loss improved from 0.03634 to 0.03608, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
            "Epoch 450/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
            "Epoch 00450: loss improved from 0.03608 to 0.03513, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "Epoch 451/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 00451: loss did not improve from 0.03513\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 0.9959\n",
            "Epoch 452/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 00452: loss improved from 0.03513 to 0.03477, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
            "Epoch 453/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
            "Epoch 00453: loss improved from 0.03477 to 0.03402, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
            "Epoch 454/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 00454: loss improved from 0.03402 to 0.03310, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0325 - val_accuracy: 1.0000\n",
            "Epoch 455/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000\n",
            "Epoch 00455: loss improved from 0.03310 to 0.03262, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
            "Epoch 456/1000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.0340 - accuracy: 0.9986\n",
            "Epoch 00456: loss did not improve from 0.03262\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.0339 - accuracy: 0.9986 - val_loss: 0.0319 - val_accuracy: 1.0000\n",
            "Epoch 457/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000\n",
            "Epoch 00457: loss improved from 0.03262 to 0.03174, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "Epoch 458/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9986\n",
            "Epoch 00458: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0329 - accuracy: 0.9986 - val_loss: 0.0364 - val_accuracy: 0.9959\n",
            "Epoch 459/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 0.9985\n",
            "Epoch 00459: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0332 - accuracy: 0.9986 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "Epoch 460/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 0.9985\n",
            "Epoch 00460: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0350 - accuracy: 0.9986 - val_loss: 0.0357 - val_accuracy: 0.9959\n",
            "Epoch 461/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 00461: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0377 - accuracy: 0.9986 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "Epoch 462/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0654 - accuracy: 0.9926\n",
            "Epoch 00462: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0659 - accuracy: 0.9918 - val_loss: 0.1118 - val_accuracy: 0.9797\n",
            "Epoch 463/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0650 - accuracy: 0.9906\n",
            "Epoch 00463: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0621 - accuracy: 0.9918 - val_loss: 0.0608 - val_accuracy: 0.9919\n",
            "Epoch 464/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0562 - accuracy: 0.9940\n",
            "Epoch 00464: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0563 - accuracy: 0.9946 - val_loss: 0.0737 - val_accuracy: 0.9919\n",
            "Epoch 465/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0795 - accuracy: 0.9911\n",
            "Epoch 00465: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0769 - accuracy: 0.9918 - val_loss: 0.0725 - val_accuracy: 0.9919\n",
            "Epoch 466/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0494 - accuracy: 0.9970\n",
            "Epoch 00466: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0484 - accuracy: 0.9973 - val_loss: 0.0822 - val_accuracy: 0.9797\n",
            "Epoch 467/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0514 - accuracy: 0.9969\n",
            "Epoch 00467: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0511 - accuracy: 0.9959 - val_loss: 0.0590 - val_accuracy: 0.9959\n",
            "Epoch 468/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0631 - accuracy: 0.9926\n",
            "Epoch 00468: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0623 - accuracy: 0.9932 - val_loss: 0.0460 - val_accuracy: 1.0000\n",
            "Epoch 469/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0449 - accuracy: 0.9985\n",
            "Epoch 00469: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0481 - accuracy: 0.9973 - val_loss: 0.0483 - val_accuracy: 0.9959\n",
            "Epoch 470/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0460 - accuracy: 0.9969\n",
            "Epoch 00470: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0455 - accuracy: 0.9973 - val_loss: 0.0449 - val_accuracy: 0.9959\n",
            "Epoch 471/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0492 - accuracy: 0.9955\n",
            "Epoch 00471: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0484 - accuracy: 0.9959 - val_loss: 0.0412 - val_accuracy: 1.0000\n",
            "Epoch 472/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0694 - accuracy: 0.9896\n",
            "Epoch 00472: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0693 - accuracy: 0.9891 - val_loss: 0.0784 - val_accuracy: 0.9837\n",
            "Epoch 473/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0498 - accuracy: 0.9970\n",
            "Epoch 00473: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0492 - accuracy: 0.9973 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
            "Epoch 474/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0523 - accuracy: 0.9955\n",
            "Epoch 00474: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0529 - accuracy: 0.9946 - val_loss: 0.0519 - val_accuracy: 0.9959\n",
            "Epoch 475/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0566 - accuracy: 0.9955\n",
            "Epoch 00475: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0604 - accuracy: 0.9932 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
            "Epoch 476/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0544 - accuracy: 0.9985\n",
            "Epoch 00476: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0543 - accuracy: 0.9973 - val_loss: 0.0481 - val_accuracy: 0.9959\n",
            "Epoch 477/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0482 - accuracy: 0.9985\n",
            "Epoch 00477: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0496 - accuracy: 0.9973 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
            "Epoch 478/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0445 - accuracy: 1.0000\n",
            "Epoch 00478: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0454 - accuracy: 0.9986 - val_loss: 0.0562 - val_accuracy: 0.9959\n",
            "Epoch 479/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0486 - accuracy: 0.9970\n",
            "Epoch 00479: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0480 - accuracy: 0.9973 - val_loss: 0.0574 - val_accuracy: 0.9959\n",
            "Epoch 480/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0439 - accuracy: 0.9985\n",
            "Epoch 00480: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0439 - accuracy: 0.9986 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
            "Epoch 481/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0455 - accuracy: 0.9985\n",
            "Epoch 00481: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0451 - accuracy: 0.9986 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
            "Epoch 482/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0430 - accuracy: 1.0000\n",
            "Epoch 00482: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 1.0000\n",
            "Epoch 483/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0449 - accuracy: 0.9985\n",
            "Epoch 00483: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0445 - accuracy: 0.9986 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
            "Epoch 484/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0424 - accuracy: 1.0000\n",
            "Epoch 00484: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 485/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0428 - accuracy: 0.9985\n",
            "Epoch 00485: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0425 - accuracy: 0.9986 - val_loss: 0.0488 - val_accuracy: 0.9959\n",
            "Epoch 486/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0468 - accuracy: 0.9953\n",
            "Epoch 00486: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0458 - accuracy: 0.9959 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
            "Epoch 487/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0447 - accuracy: 0.9985\n",
            "Epoch 00487: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0443 - accuracy: 0.9986 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
            "Epoch 488/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9905\n",
            "Epoch 00488: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0699 - accuracy: 0.9905 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
            "Epoch 489/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0544 - accuracy: 0.9953\n",
            "Epoch 00489: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0557 - accuracy: 0.9946 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
            "Epoch 490/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0468 - accuracy: 0.9984\n",
            "Epoch 00490: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0461 - accuracy: 0.9986 - val_loss: 0.0509 - val_accuracy: 0.9959\n",
            "Epoch 491/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0538 - accuracy: 0.9940\n",
            "Epoch 00491: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0536 - accuracy: 0.9946 - val_loss: 0.0733 - val_accuracy: 0.9878\n",
            "Epoch 492/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0921 - accuracy: 0.9866\n",
            "Epoch 00492: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0886 - accuracy: 0.9878 - val_loss: 0.0688 - val_accuracy: 0.9959\n",
            "Epoch 493/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0537 - accuracy: 0.9955\n",
            "Epoch 00493: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0553 - accuracy: 0.9946 - val_loss: 0.0533 - val_accuracy: 0.9959\n",
            "Epoch 494/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0505 - accuracy: 0.9940\n",
            "Epoch 00494: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0500 - accuracy: 0.9946 - val_loss: 0.0567 - val_accuracy: 0.9959\n",
            "Epoch 495/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0476 - accuracy: 0.9969\n",
            "Epoch 00495: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0485 - accuracy: 0.9959 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
            "Epoch 496/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0450 - accuracy: 0.9985\n",
            "Epoch 00496: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0447 - accuracy: 0.9986 - val_loss: 0.0553 - val_accuracy: 0.9959\n",
            "Epoch 497/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0434 - accuracy: 1.0000\n",
            "Epoch 00497: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 0.9959\n",
            "Epoch 498/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0443 - accuracy: 0.9984\n",
            "Epoch 00498: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0479 - accuracy: 0.9959 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
            "Epoch 499/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0470 - accuracy: 0.9970\n",
            "Epoch 00499: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0468 - accuracy: 0.9973 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
            "Epoch 500/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0457 - accuracy: 0.9985\n",
            "Epoch 00500: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0452 - accuracy: 0.9986 - val_loss: 0.0472 - val_accuracy: 0.9959\n",
            "Epoch 501/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0421 - accuracy: 1.0000\n",
            "Epoch 00501: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9959\n",
            "Epoch 502/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0472 - accuracy: 0.9970\n",
            "Epoch 00502: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0466 - accuracy: 0.9973 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 503/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0434 - accuracy: 0.9985\n",
            "Epoch 00503: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0430 - accuracy: 0.9986 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "Epoch 504/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0404 - accuracy: 1.0000\n",
            "Epoch 00504: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
            "Epoch 505/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0386 - accuracy: 1.0000\n",
            "Epoch 00505: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
            "Epoch 506/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 00506: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "Epoch 507/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0402 - accuracy: 0.9985\n",
            "Epoch 00507: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0413 - accuracy: 0.9973 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "Epoch 508/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n",
            "Epoch 00508: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
            "Epoch 509/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0444 - accuracy: 0.9955\n",
            "Epoch 00509: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0438 - accuracy: 0.9959 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
            "Epoch 510/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0731 - accuracy: 0.9896\n",
            "Epoch 00510: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0720 - accuracy: 0.9905 - val_loss: 0.0885 - val_accuracy: 0.9837\n",
            "Epoch 511/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0713 - accuracy: 0.9881\n",
            "Epoch 00511: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0687 - accuracy: 0.9891 - val_loss: 0.0584 - val_accuracy: 0.9959\n",
            "Epoch 512/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0656 - accuracy: 0.9926\n",
            "Epoch 00512: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0669 - accuracy: 0.9918 - val_loss: 0.0724 - val_accuracy: 0.9919\n",
            "Epoch 513/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0590 - accuracy: 0.9985\n",
            "Epoch 00513: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0577 - accuracy: 0.9986 - val_loss: 0.0467 - val_accuracy: 0.9959\n",
            "Epoch 514/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0484 - accuracy: 0.9984\n",
            "Epoch 00514: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0481 - accuracy: 0.9986 - val_loss: 0.0476 - val_accuracy: 0.9959\n",
            "Epoch 515/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0519 - accuracy: 0.9955\n",
            "Epoch 00515: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0516 - accuracy: 0.9959 - val_loss: 0.0529 - val_accuracy: 0.9959\n",
            "Epoch 516/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0438 - accuracy: 1.0000\n",
            "Epoch 00516: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0447 - accuracy: 0.9986 - val_loss: 0.0442 - val_accuracy: 1.0000\n",
            "Epoch 517/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0440 - accuracy: 0.9985\n",
            "Epoch 00517: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0437 - accuracy: 0.9986 - val_loss: 0.0649 - val_accuracy: 0.9919\n",
            "Epoch 518/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0451 - accuracy: 1.0000\n",
            "Epoch 00518: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0457 - accuracy: 0.9986 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
            "Epoch 519/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
            "Epoch 00519: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 520/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0397 - accuracy: 1.0000\n",
            "Epoch 00520: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 521/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0411 - accuracy: 0.9985\n",
            "Epoch 00521: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0410 - accuracy: 0.9986 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
            "Epoch 522/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
            "Epoch 00522: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
            "Epoch 523/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0415 - accuracy: 0.9985\n",
            "Epoch 00523: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0412 - accuracy: 0.9986 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
            "Epoch 524/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 00524: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "Epoch 525/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000\n",
            "Epoch 00525: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
            "Epoch 526/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n",
            "Epoch 00526: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "Epoch 527/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
            "Epoch 00527: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
            "Epoch 528/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 00528: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 529/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 00529: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "Epoch 530/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0350 - accuracy: 1.0000\n",
            "Epoch 00530: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9959\n",
            "Epoch 531/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 00531: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 0.9959\n",
            "Epoch 532/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 00532: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
            "Epoch 533/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0343 - accuracy: 0.9985\n",
            "Epoch 00533: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0342 - accuracy: 0.9986 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
            "Epoch 534/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000\n",
            "Epoch 00534: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
            "Epoch 535/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 00535: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0476 - val_accuracy: 0.9959\n",
            "Epoch 536/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 1.0000\n",
            "Epoch 00536: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
            "Epoch 537/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 00537: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9959\n",
            "Epoch 538/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0337 - accuracy: 0.9985\n",
            "Epoch 00538: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0386 - accuracy: 0.9973 - val_loss: 0.0415 - val_accuracy: 0.9959\n",
            "Epoch 539/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0354 - accuracy: 1.0000\n",
            "Epoch 00539: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
            "Epoch 540/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
            "Epoch 00540: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
            "Epoch 541/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0491 - accuracy: 0.9926\n",
            "Epoch 00541: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0484 - accuracy: 0.9932 - val_loss: 0.0596 - val_accuracy: 0.9919\n",
            "Epoch 542/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0528 - accuracy: 0.9940\n",
            "Epoch 00542: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0541 - accuracy: 0.9932 - val_loss: 0.0708 - val_accuracy: 0.9959\n",
            "Epoch 543/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0537 - accuracy: 0.9926\n",
            "Epoch 00543: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0521 - accuracy: 0.9932 - val_loss: 0.0418 - val_accuracy: 1.0000\n",
            "Epoch 544/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0559 - accuracy: 0.9937\n",
            "Epoch 00544: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0562 - accuracy: 0.9932 - val_loss: 0.0925 - val_accuracy: 0.9797\n",
            "Epoch 545/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0556 - accuracy: 0.9937\n",
            "Epoch 00545: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0668 - accuracy: 0.9918 - val_loss: 0.0447 - val_accuracy: 0.9959\n",
            "Epoch 546/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0607 - accuracy: 0.9970\n",
            "Epoch 00546: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0593 - accuracy: 0.9973 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
            "Epoch 547/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0447 - accuracy: 0.9970\n",
            "Epoch 00547: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0443 - accuracy: 0.9973 - val_loss: 0.0639 - val_accuracy: 0.9959\n",
            "Epoch 548/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0387 - accuracy: 0.9985\n",
            "Epoch 00548: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0400 - accuracy: 0.9973 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
            "Epoch 549/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0385 - accuracy: 1.0000\n",
            "Epoch 00549: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 0.9959\n",
            "Epoch 550/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 00550: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 1.0000\n",
            "Epoch 551/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
            "Epoch 00551: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9959\n",
            "Epoch 552/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00552: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "Epoch 553/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00553: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 0.9959\n",
            "Epoch 554/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9986\n",
            "Epoch 00554: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0466 - accuracy: 0.9986 - val_loss: 0.0706 - val_accuracy: 0.9878\n",
            "Epoch 555/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0423 - accuracy: 0.9955\n",
            "Epoch 00555: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0417 - accuracy: 0.9959 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 556/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0458 - accuracy: 0.9970\n",
            "Epoch 00556: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0492 - accuracy: 0.9959 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
            "Epoch 557/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0561 - accuracy: 0.9940\n",
            "Epoch 00557: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0552 - accuracy: 0.9946 - val_loss: 0.0461 - val_accuracy: 0.9959\n",
            "Epoch 558/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0472 - accuracy: 0.9985\n",
            "Epoch 00558: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0467 - accuracy: 0.9986 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
            "Epoch 559/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0643 - accuracy: 0.9881\n",
            "Epoch 00559: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0641 - accuracy: 0.9878 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
            "Epoch 560/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0422 - accuracy: 1.0000\n",
            "Epoch 00560: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0423 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "Epoch 561/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0423 - accuracy: 0.9970\n",
            "Epoch 00561: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0424 - accuracy: 0.9973 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
            "Epoch 562/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0394 - accuracy: 1.0000\n",
            "Epoch 00562: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 563/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0378 - accuracy: 1.0000\n",
            "Epoch 00563: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
            "Epoch 564/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 1.0000\n",
            "Epoch 00564: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
            "Epoch 565/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
            "Epoch 00565: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
            "Epoch 566/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 0.9985\n",
            "Epoch 00566: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0379 - accuracy: 0.9986 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "Epoch 567/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
            "Epoch 00567: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 0.9959\n",
            "Epoch 568/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 00568: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9959\n",
            "Epoch 569/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
            "Epoch 00569: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9959\n",
            "Epoch 570/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000\n",
            "Epoch 00570: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 0.9959\n",
            "Epoch 571/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 00571: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9959\n",
            "Epoch 572/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 00572: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 0.9959\n",
            "Epoch 573/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 00573: loss did not improve from 0.03174\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9959\n",
            "Epoch 574/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 00574: loss improved from 0.03174 to 0.03169, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.0346 - val_accuracy: 0.9959\n",
            "Epoch 575/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 00575: loss improved from 0.03169 to 0.03117, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 0.9959\n",
            "Epoch 576/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 00576: loss improved from 0.03117 to 0.03083, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 0.9959\n",
            "Epoch 577/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000\n",
            "Epoch 00577: loss improved from 0.03083 to 0.02999, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9959\n",
            "Epoch 578/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0299 - accuracy: 1.0000\n",
            "Epoch 00578: loss improved from 0.02999 to 0.02987, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0319 - val_accuracy: 1.0000\n",
            "Epoch 579/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0331 - accuracy: 0.9970\n",
            "Epoch 00579: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0327 - accuracy: 0.9973 - val_loss: 0.0471 - val_accuracy: 0.9959\n",
            "Epoch 580/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 00580: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "Epoch 581/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 00581: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 582/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 00582: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 583/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000\n",
            "Epoch 00583: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
            "Epoch 584/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0361 - accuracy: 0.9970\n",
            "Epoch 00584: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0367 - accuracy: 0.9973 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "Epoch 585/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0406 - accuracy: 0.9955\n",
            "Epoch 00585: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0404 - accuracy: 0.9959 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
            "Epoch 586/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0366 - accuracy: 0.9970\n",
            "Epoch 00586: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0363 - accuracy: 0.9973 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
            "Epoch 587/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0380 - accuracy: 0.9970\n",
            "Epoch 00587: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0378 - accuracy: 0.9973 - val_loss: 0.1365 - val_accuracy: 0.9634\n",
            "Epoch 588/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0546 - accuracy: 0.9926\n",
            "Epoch 00588: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0554 - accuracy: 0.9918 - val_loss: 0.0483 - val_accuracy: 0.9959\n",
            "Epoch 589/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0728 - accuracy: 0.9881\n",
            "Epoch 00589: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0701 - accuracy: 0.9891 - val_loss: 0.0872 - val_accuracy: 0.9837\n",
            "Epoch 590/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0939 - accuracy: 0.9777\n",
            "Epoch 00590: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0902 - accuracy: 0.9796 - val_loss: 0.0541 - val_accuracy: 0.9959\n",
            "Epoch 591/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0610 - accuracy: 0.9896\n",
            "Epoch 00591: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0600 - accuracy: 0.9905 - val_loss: 0.0654 - val_accuracy: 0.9837\n",
            "Epoch 592/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0620 - accuracy: 0.9881\n",
            "Epoch 00592: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0611 - accuracy: 0.9891 - val_loss: 0.0513 - val_accuracy: 0.9919\n",
            "Epoch 593/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0462 - accuracy: 0.9970\n",
            "Epoch 00593: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0457 - accuracy: 0.9973 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
            "Epoch 594/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0422 - accuracy: 1.0000\n",
            "Epoch 00594: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
            "Epoch 595/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0546 - accuracy: 0.9969\n",
            "Epoch 00595: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0538 - accuracy: 0.9973 - val_loss: 0.0822 - val_accuracy: 0.9919\n",
            "Epoch 596/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0611 - accuracy: 0.9896\n",
            "Epoch 00596: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0604 - accuracy: 0.9891 - val_loss: 0.0553 - val_accuracy: 0.9959\n",
            "Epoch 597/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0548 - accuracy: 0.9940\n",
            "Epoch 00597: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0543 - accuracy: 0.9946 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
            "Epoch 598/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0521 - accuracy: 0.9985\n",
            "Epoch 00598: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0513 - accuracy: 0.9986 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "Epoch 599/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0443 - accuracy: 0.9985\n",
            "Epoch 00599: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0533 - accuracy: 0.9973 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
            "Epoch 600/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0478 - accuracy: 0.9985\n",
            "Epoch 00600: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0511 - accuracy: 0.9973 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
            "Epoch 601/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0481 - accuracy: 0.9985\n",
            "Epoch 00601: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0485 - accuracy: 0.9973 - val_loss: 0.0520 - val_accuracy: 0.9959\n",
            "Epoch 602/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0451 - accuracy: 1.0000\n",
            "Epoch 00602: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 0.9959\n",
            "Epoch 603/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0398 - accuracy: 1.0000\n",
            "Epoch 00603: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0424 - accuracy: 0.9986 - val_loss: 0.0419 - val_accuracy: 1.0000\n",
            "Epoch 604/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0438 - accuracy: 0.9985\n",
            "Epoch 00604: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0437 - accuracy: 0.9986 - val_loss: 0.0554 - val_accuracy: 0.9959\n",
            "Epoch 605/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0403 - accuracy: 1.0000\n",
            "Epoch 00605: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.0615 - val_accuracy: 0.9959\n",
            "Epoch 606/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 00606: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9959\n",
            "Epoch 607/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 00607: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 0.9959\n",
            "Epoch 608/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000\n",
            "Epoch 00608: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 0.9959\n",
            "Epoch 609/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000\n",
            "Epoch 00609: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0380 - accuracy: 0.9986 - val_loss: 0.0427 - val_accuracy: 0.9959\n",
            "Epoch 610/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0385 - accuracy: 0.9984\n",
            "Epoch 00610: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0386 - accuracy: 0.9986 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 611/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0781 - accuracy: 0.9866\n",
            "Epoch 00611: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0779 - accuracy: 0.9864 - val_loss: 0.0614 - val_accuracy: 0.9919\n",
            "Epoch 612/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0541 - accuracy: 0.9970\n",
            "Epoch 00612: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0542 - accuracy: 0.9973 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 613/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0433 - accuracy: 1.0000\n",
            "Epoch 00613: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9959\n",
            "Epoch 614/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
            "Epoch 00614: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 615/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0392 - accuracy: 1.0000\n",
            "Epoch 00615: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "Epoch 616/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 1.0000\n",
            "Epoch 00616: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
            "Epoch 617/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
            "Epoch 00617: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "Epoch 618/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0390 - accuracy: 0.9969\n",
            "Epoch 00618: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0406 - accuracy: 0.9959 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
            "Epoch 619/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0389 - accuracy: 1.0000\n",
            "Epoch 00619: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0433 - val_accuracy: 1.0000\n",
            "Epoch 620/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0417 - accuracy: 0.9985\n",
            "Epoch 00620: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0411 - accuracy: 0.9986 - val_loss: 0.0483 - val_accuracy: 0.9959\n",
            "Epoch 621/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0387 - accuracy: 0.9985\n",
            "Epoch 00621: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0385 - accuracy: 0.9986 - val_loss: 0.0442 - val_accuracy: 0.9959\n",
            "Epoch 622/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00622: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "Epoch 623/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 00623: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
            "Epoch 624/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0465 - accuracy: 0.9970\n",
            "Epoch 00624: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0463 - accuracy: 0.9973 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "Epoch 625/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0426 - accuracy: 0.9955\n",
            "Epoch 00625: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0421 - accuracy: 0.9959 - val_loss: 0.0447 - val_accuracy: 0.9919\n",
            "Epoch 626/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0399 - accuracy: 0.9985\n",
            "Epoch 00626: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0394 - accuracy: 0.9986 - val_loss: 0.0399 - val_accuracy: 0.9959\n",
            "Epoch 627/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0431 - accuracy: 0.9955\n",
            "Epoch 00627: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0424 - accuracy: 0.9959 - val_loss: 0.0445 - val_accuracy: 0.9959\n",
            "Epoch 628/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0472 - accuracy: 0.9970\n",
            "Epoch 00628: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0465 - accuracy: 0.9973 - val_loss: 0.0734 - val_accuracy: 0.9919\n",
            "Epoch 629/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0656 - accuracy: 0.9896\n",
            "Epoch 00629: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0630 - accuracy: 0.9905 - val_loss: 0.0517 - val_accuracy: 0.9959\n",
            "Epoch 630/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0534 - accuracy: 0.9926\n",
            "Epoch 00630: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0526 - accuracy: 0.9932 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
            "Epoch 631/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0466 - accuracy: 0.9955\n",
            "Epoch 00631: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0459 - accuracy: 0.9959 - val_loss: 0.0451 - val_accuracy: 0.9959\n",
            "Epoch 632/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0472 - accuracy: 0.9970\n",
            "Epoch 00632: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0476 - accuracy: 0.9959 - val_loss: 0.0548 - val_accuracy: 0.9959\n",
            "Epoch 633/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0395 - accuracy: 1.0000\n",
            "Epoch 00633: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
            "Epoch 634/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0405 - accuracy: 0.9985\n",
            "Epoch 00634: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0401 - accuracy: 0.9986 - val_loss: 0.0440 - val_accuracy: 0.9959\n",
            "Epoch 635/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0599 - accuracy: 0.9955\n",
            "Epoch 00635: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0619 - accuracy: 0.9946 - val_loss: 0.0427 - val_accuracy: 0.9959\n",
            "Epoch 636/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0472 - accuracy: 0.9940\n",
            "Epoch 00636: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0466 - accuracy: 0.9946 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "Epoch 637/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0539 - accuracy: 0.9940\n",
            "Epoch 00637: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0525 - accuracy: 0.9946 - val_loss: 0.0511 - val_accuracy: 0.9959\n",
            "Epoch 638/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0575 - accuracy: 0.9911\n",
            "Epoch 00638: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0564 - accuracy: 0.9918 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
            "Epoch 639/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0530 - accuracy: 0.9970\n",
            "Epoch 00639: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0523 - accuracy: 0.9973 - val_loss: 0.0463 - val_accuracy: 0.9959\n",
            "Epoch 640/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 00640: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 641/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
            "Epoch 00641: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "Epoch 642/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0375 - accuracy: 1.0000\n",
            "Epoch 00642: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
            "Epoch 643/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0374 - accuracy: 1.0000\n",
            "Epoch 00643: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 644/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 00644: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9959\n",
            "Epoch 645/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0373 - accuracy: 1.0000\n",
            "Epoch 00645: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
            "Epoch 646/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0369 - accuracy: 1.0000\n",
            "Epoch 00646: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
            "Epoch 647/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00647: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "Epoch 648/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0465 - accuracy: 0.9970\n",
            "Epoch 00648: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0455 - accuracy: 0.9973 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "Epoch 649/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0371 - accuracy: 0.9985\n",
            "Epoch 00649: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0371 - accuracy: 0.9986 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
            "Epoch 650/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 00650: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "Epoch 651/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 00651: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
            "Epoch 652/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 00652: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 653/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000\n",
            "Epoch 00653: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "Epoch 654/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 00654: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
            "Epoch 655/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 00655: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "Epoch 656/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 00656: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 657/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 00657: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0319 - val_accuracy: 1.0000\n",
            "Epoch 658/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 00658: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 659/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 00659: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "Epoch 660/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 00660: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
            "Epoch 661/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000\n",
            "Epoch 00661: loss did not improve from 0.02987\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
            "Epoch 662/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000\n",
            "Epoch 00662: loss improved from 0.02987 to 0.02987, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
            "Epoch 663/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000\n",
            "Epoch 00663: loss improved from 0.02987 to 0.02892, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
            "Epoch 664/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
            "Epoch 00664: loss improved from 0.02892 to 0.02862, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
            "Epoch 665/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
            "Epoch 00665: loss improved from 0.02862 to 0.02855, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
            "Epoch 666/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0281 - accuracy: 1.0000\n",
            "Epoch 00666: loss improved from 0.02855 to 0.02809, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "Epoch 667/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 00667: loss improved from 0.02809 to 0.02718, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "Epoch 668/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 00668: loss improved from 0.02718 to 0.02708, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
            "Epoch 669/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000\n",
            "Epoch 00669: loss improved from 0.02708 to 0.02639, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
            "Epoch 670/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0266 - accuracy: 1.0000\n",
            "Epoch 00670: loss did not improve from 0.02639\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
            "Epoch 671/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 00671: loss improved from 0.02639 to 0.02589, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
            "Epoch 672/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 00672: loss did not improve from 0.02589\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 0.9959\n",
            "Epoch 673/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0270 - accuracy: 1.0000\n",
            "Epoch 00673: loss did not improve from 0.02589\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
            "Epoch 674/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
            "Epoch 00674: loss did not improve from 0.02589\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
            "Epoch 675/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0252 - accuracy: 1.0000\n",
            "Epoch 00675: loss improved from 0.02589 to 0.02512, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
            "Epoch 676/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
            "Epoch 00676: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
            "Epoch 677/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0302 - accuracy: 0.9955\n",
            "Epoch 00677: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0300 - accuracy: 0.9959 - val_loss: 0.0715 - val_accuracy: 0.9837\n",
            "Epoch 678/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0431 - accuracy: 0.9955\n",
            "Epoch 00678: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0427 - accuracy: 0.9959 - val_loss: 0.0973 - val_accuracy: 0.9837\n",
            "Epoch 679/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0558 - accuracy: 0.9866\n",
            "Epoch 00679: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0613 - accuracy: 0.9823 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
            "Epoch 680/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0666 - accuracy: 0.9792\n",
            "Epoch 00680: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0650 - accuracy: 0.9810 - val_loss: 0.0486 - val_accuracy: 0.9919\n",
            "Epoch 681/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0637 - accuracy: 0.9911\n",
            "Epoch 00681: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0649 - accuracy: 0.9905 - val_loss: 0.0798 - val_accuracy: 0.9797\n",
            "Epoch 682/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0558 - accuracy: 0.9911\n",
            "Epoch 00682: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0549 - accuracy: 0.9918 - val_loss: 0.0801 - val_accuracy: 0.9878\n",
            "Epoch 683/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0488 - accuracy: 0.9955\n",
            "Epoch 00683: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0478 - accuracy: 0.9959 - val_loss: 0.0462 - val_accuracy: 0.9959\n",
            "Epoch 684/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
            "Epoch 00684: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 0.9959\n",
            "Epoch 685/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0409 - accuracy: 0.9985\n",
            "Epoch 00685: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0437 - accuracy: 0.9973 - val_loss: 0.1392 - val_accuracy: 0.9756\n",
            "Epoch 686/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1072 - accuracy: 0.9777\n",
            "Epoch 00686: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1033 - accuracy: 0.9782 - val_loss: 0.0632 - val_accuracy: 0.9919\n",
            "Epoch 687/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0573 - accuracy: 0.9955\n",
            "Epoch 00687: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0561 - accuracy: 0.9959 - val_loss: 0.0641 - val_accuracy: 0.9837\n",
            "Epoch 688/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 00688: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9878\n",
            "Epoch 689/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 00689: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9959\n",
            "Epoch 690/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 00690: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0537 - val_accuracy: 0.9959\n",
            "Epoch 691/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
            "Epoch 00691: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9959\n",
            "Epoch 692/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0348 - accuracy: 1.0000\n",
            "Epoch 00692: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9959\n",
            "Epoch 693/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 00693: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9959\n",
            "Epoch 694/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0350 - accuracy: 1.0000\n",
            "Epoch 00694: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.0381 - val_accuracy: 0.9959\n",
            "Epoch 695/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 00695: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
            "Epoch 696/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0373 - accuracy: 0.9985\n",
            "Epoch 00696: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0369 - accuracy: 0.9986 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
            "Epoch 697/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0486 - accuracy: 0.9926\n",
            "Epoch 00697: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0480 - accuracy: 0.9932 - val_loss: 0.0452 - val_accuracy: 0.9959\n",
            "Epoch 698/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0479 - accuracy: 0.9955\n",
            "Epoch 00698: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0470 - accuracy: 0.9959 - val_loss: 0.0437 - val_accuracy: 0.9959\n",
            "Epoch 699/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0502 - accuracy: 0.9940\n",
            "Epoch 00699: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0490 - accuracy: 0.9946 - val_loss: 0.0491 - val_accuracy: 0.9959\n",
            "Epoch 700/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0488 - accuracy: 0.9969\n",
            "Epoch 00700: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0490 - accuracy: 0.9973 - val_loss: 0.0501 - val_accuracy: 0.9919\n",
            "Epoch 701/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0704 - accuracy: 0.9911\n",
            "Epoch 00701: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0692 - accuracy: 0.9905 - val_loss: 0.1333 - val_accuracy: 0.9756\n",
            "Epoch 702/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0613 - accuracy: 0.9926\n",
            "Epoch 00702: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0595 - accuracy: 0.9932 - val_loss: 0.0559 - val_accuracy: 0.9919\n",
            "Epoch 703/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0576 - accuracy: 0.9911\n",
            "Epoch 00703: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0560 - accuracy: 0.9918 - val_loss: 0.0643 - val_accuracy: 0.9878\n",
            "Epoch 704/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0458 - accuracy: 0.9970\n",
            "Epoch 00704: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0452 - accuracy: 0.9973 - val_loss: 0.0732 - val_accuracy: 0.9878\n",
            "Epoch 705/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
            "Epoch 00705: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 0.9919\n",
            "Epoch 706/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 00706: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9959\n",
            "Epoch 707/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0453 - accuracy: 0.9970\n",
            "Epoch 00707: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0446 - accuracy: 0.9973 - val_loss: 0.0552 - val_accuracy: 0.9959\n",
            "Epoch 708/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 00708: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.0532 - val_accuracy: 0.9959\n",
            "Epoch 709/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0373 - accuracy: 1.0000\n",
            "Epoch 00709: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9959\n",
            "Epoch 710/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
            "Epoch 00710: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 0.9959\n",
            "Epoch 711/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 00711: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.9959\n",
            "Epoch 712/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 1.0000\n",
            "Epoch 00712: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9959\n",
            "Epoch 713/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0363 - accuracy: 0.9985\n",
            "Epoch 00713: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0406 - accuracy: 0.9973 - val_loss: 0.0522 - val_accuracy: 0.9919\n",
            "Epoch 714/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9986\n",
            "Epoch 00714: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0361 - accuracy: 0.9986 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
            "Epoch 715/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000\n",
            "Epoch 00715: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "Epoch 716/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0354 - accuracy: 0.9985\n",
            "Epoch 00716: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0352 - accuracy: 0.9986 - val_loss: 0.0437 - val_accuracy: 0.9959\n",
            "Epoch 717/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000\n",
            "Epoch 00717: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
            "Epoch 718/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 00718: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
            "Epoch 719/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 00719: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 0.9959\n",
            "Epoch 720/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000\n",
            "Epoch 00720: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9959\n",
            "Epoch 721/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0322 - accuracy: 1.0000\n",
            "Epoch 00721: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 0.9959\n",
            "Epoch 722/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 00722: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 0.9959\n",
            "Epoch 723/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000\n",
            "Epoch 00723: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 0.9959\n",
            "Epoch 724/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 00724: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 725/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 00725: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
            "Epoch 726/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
            "Epoch 00726: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 727/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0309 - accuracy: 0.9985\n",
            "Epoch 00727: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0307 - accuracy: 0.9986 - val_loss: 0.0373 - val_accuracy: 0.9959\n",
            "Epoch 728/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0324 - accuracy: 0.9984\n",
            "Epoch 00728: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0325 - accuracy: 0.9986 - val_loss: 0.0356 - val_accuracy: 0.9959\n",
            "Epoch 729/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0308 - accuracy: 0.9985\n",
            "Epoch 00729: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0306 - accuracy: 0.9986 - val_loss: 0.0339 - val_accuracy: 0.9959\n",
            "Epoch 730/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0281 - accuracy: 1.0000\n",
            "Epoch 00730: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 731/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
            "Epoch 00731: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "Epoch 732/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 00732: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 0.9959\n",
            "Epoch 733/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0307 - accuracy: 0.9984\n",
            "Epoch 00733: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0311 - accuracy: 0.9986 - val_loss: 0.0488 - val_accuracy: 0.9919\n",
            "Epoch 734/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0382 - accuracy: 0.9970\n",
            "Epoch 00734: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0424 - accuracy: 0.9959 - val_loss: 0.0468 - val_accuracy: 0.9878\n",
            "Epoch 735/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0653 - accuracy: 0.9911\n",
            "Epoch 00735: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0627 - accuracy: 0.9918 - val_loss: 0.0787 - val_accuracy: 0.9878\n",
            "Epoch 736/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0571 - accuracy: 0.9911\n",
            "Epoch 00736: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0551 - accuracy: 0.9918 - val_loss: 0.0426 - val_accuracy: 0.9959\n",
            "Epoch 737/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0453 - accuracy: 0.9970\n",
            "Epoch 00737: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0443 - accuracy: 0.9973 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 738/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000\n",
            "Epoch 00738: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0389 - accuracy: 0.9986 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "Epoch 739/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0407 - accuracy: 0.9985\n",
            "Epoch 00739: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0404 - accuracy: 0.9986 - val_loss: 0.0614 - val_accuracy: 0.9878\n",
            "Epoch 740/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0457 - accuracy: 0.9970\n",
            "Epoch 00740: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0446 - accuracy: 0.9973 - val_loss: 0.0478 - val_accuracy: 0.9959\n",
            "Epoch 741/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0687 - accuracy: 0.9911\n",
            "Epoch 00741: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0716 - accuracy: 0.9905 - val_loss: 0.0450 - val_accuracy: 0.9959\n",
            "Epoch 742/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0554 - accuracy: 0.9911\n",
            "Epoch 00742: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0548 - accuracy: 0.9918 - val_loss: 0.0438 - val_accuracy: 0.9959\n",
            "Epoch 743/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0437 - accuracy: 0.9955\n",
            "Epoch 00743: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0429 - accuracy: 0.9959 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
            "Epoch 744/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0398 - accuracy: 0.9984\n",
            "Epoch 00744: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0390 - accuracy: 0.9986 - val_loss: 0.0385 - val_accuracy: 0.9959\n",
            "Epoch 745/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00745: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0355 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9959\n",
            "Epoch 746/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0346 - accuracy: 1.0000\n",
            "Epoch 00746: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 0.9959\n",
            "Epoch 747/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0334 - accuracy: 1.0000\n",
            "Epoch 00747: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0339 - val_accuracy: 1.0000\n",
            "Epoch 748/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 00748: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
            "Epoch 749/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0341 - accuracy: 0.9985\n",
            "Epoch 00749: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0373 - accuracy: 0.9973 - val_loss: 0.0407 - val_accuracy: 0.9959\n",
            "Epoch 750/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0355 - accuracy: 0.9985\n",
            "Epoch 00750: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0353 - accuracy: 0.9986 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
            "Epoch 751/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0369 - accuracy: 0.9985\n",
            "Epoch 00751: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0368 - accuracy: 0.9986 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
            "Epoch 752/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0468 - accuracy: 0.9955\n",
            "Epoch 00752: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0511 - accuracy: 0.9946 - val_loss: 0.0508 - val_accuracy: 0.9919\n",
            "Epoch 753/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0402 - accuracy: 0.9955\n",
            "Epoch 00753: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0410 - accuracy: 0.9946 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 754/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0352 - accuracy: 0.9985\n",
            "Epoch 00754: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0351 - accuracy: 0.9986 - val_loss: 0.0421 - val_accuracy: 0.9959\n",
            "Epoch 755/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
            "Epoch 00755: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
            "Epoch 756/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0388 - accuracy: 0.9985\n",
            "Epoch 00756: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0390 - accuracy: 0.9986 - val_loss: 0.0549 - val_accuracy: 0.9959\n",
            "Epoch 757/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 00757: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 0.9919\n",
            "Epoch 758/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0406 - accuracy: 0.9970\n",
            "Epoch 00758: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0399 - accuracy: 0.9973 - val_loss: 0.0612 - val_accuracy: 0.9878\n",
            "Epoch 759/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0412 - accuracy: 0.9953\n",
            "Epoch 00759: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0402 - accuracy: 0.9959 - val_loss: 0.0546 - val_accuracy: 0.9959\n",
            "Epoch 760/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0423 - accuracy: 0.9970\n",
            "Epoch 00760: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0415 - accuracy: 0.9973 - val_loss: 0.0691 - val_accuracy: 0.9837\n",
            "Epoch 761/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0432 - accuracy: 0.9984\n",
            "Epoch 00761: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0422 - accuracy: 0.9986 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
            "Epoch 762/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0349 - accuracy: 0.9985\n",
            "Epoch 00762: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0354 - accuracy: 0.9986 - val_loss: 0.0490 - val_accuracy: 0.9959\n",
            "Epoch 763/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
            "Epoch 00763: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 0.9959\n",
            "Epoch 764/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0363 - accuracy: 0.9970\n",
            "Epoch 00764: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0369 - accuracy: 0.9959 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
            "Epoch 765/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 0.9985\n",
            "Epoch 00765: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0357 - accuracy: 0.9986 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
            "Epoch 766/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0348 - accuracy: 1.0000\n",
            "Epoch 00766: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
            "Epoch 767/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 00767: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
            "Epoch 768/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0371 - accuracy: 0.9969\n",
            "Epoch 00768: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0401 - accuracy: 0.9959 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 769/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000\n",
            "Epoch 00769: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
            "Epoch 770/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000\n",
            "Epoch 00770: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
            "Epoch 771/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0324 - accuracy: 1.0000\n",
            "Epoch 00771: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "Epoch 772/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0314 - accuracy: 1.0000\n",
            "Epoch 00772: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 773/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000\n",
            "Epoch 00773: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "Epoch 774/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000\n",
            "Epoch 00774: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0323 - accuracy: 0.9986 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
            "Epoch 775/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 00775: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "Epoch 776/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000\n",
            "Epoch 00776: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
            "Epoch 777/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000\n",
            "Epoch 00777: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "Epoch 778/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 00778: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 0.9959\n",
            "Epoch 779/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 00779: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 0.9959\n",
            "Epoch 780/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000\n",
            "Epoch 00780: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
            "Epoch 781/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 00781: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 0.9959\n",
            "Epoch 782/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0280 - accuracy: 1.0000\n",
            "Epoch 00782: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "Epoch 783/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0280 - accuracy: 1.0000\n",
            "Epoch 00783: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
            "Epoch 784/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
            "Epoch 00784: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
            "Epoch 785/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
            "Epoch 00785: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "Epoch 786/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0273 - accuracy: 1.0000\n",
            "Epoch 00786: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
            "Epoch 787/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0260 - accuracy: 1.0000\n",
            "Epoch 00787: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
            "Epoch 788/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0363 - accuracy: 0.9955\n",
            "Epoch 00788: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0360 - accuracy: 0.9959 - val_loss: 0.0385 - val_accuracy: 0.9959\n",
            "Epoch 789/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0324 - accuracy: 0.9985\n",
            "Epoch 00789: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0359 - accuracy: 0.9973 - val_loss: 0.0326 - val_accuracy: 0.9959\n",
            "Epoch 790/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 00790: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 0.9959\n",
            "Epoch 791/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000\n",
            "Epoch 00791: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 0.9959\n",
            "Epoch 792/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
            "Epoch 00792: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
            "Epoch 793/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0276 - accuracy: 1.0000\n",
            "Epoch 00793: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "Epoch 794/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 00794: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "Epoch 795/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0308 - accuracy: 0.9970\n",
            "Epoch 00795: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0312 - accuracy: 0.9973 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 796/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 00796: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "Epoch 797/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
            "Epoch 00797: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
            "Epoch 798/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0302 - accuracy: 0.9985\n",
            "Epoch 00798: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0298 - accuracy: 0.9986 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
            "Epoch 799/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0335 - accuracy: 0.9970\n",
            "Epoch 00799: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0367 - accuracy: 0.9959 - val_loss: 0.0392 - val_accuracy: 0.9959\n",
            "Epoch 800/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0302 - accuracy: 0.9985\n",
            "Epoch 00800: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0308 - accuracy: 0.9986 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 801/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0709 - accuracy: 0.9866\n",
            "Epoch 00801: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0731 - accuracy: 0.9864 - val_loss: 0.0561 - val_accuracy: 0.9919\n",
            "Epoch 802/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0485 - accuracy: 0.9922\n",
            "Epoch 00802: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0465 - accuracy: 0.9932 - val_loss: 0.0521 - val_accuracy: 0.9959\n",
            "Epoch 803/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0650 - accuracy: 0.9881\n",
            "Epoch 00803: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0646 - accuracy: 0.9878 - val_loss: 0.0639 - val_accuracy: 0.9919\n",
            "Epoch 804/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0612 - accuracy: 0.9911\n",
            "Epoch 00804: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0592 - accuracy: 0.9918 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
            "Epoch 805/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0374 - accuracy: 1.0000\n",
            "Epoch 00805: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 0.9959\n",
            "Epoch 806/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0376 - accuracy: 0.9985\n",
            "Epoch 00806: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0372 - accuracy: 0.9986 - val_loss: 0.0422 - val_accuracy: 0.9959\n",
            "Epoch 807/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 00807: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.9959\n",
            "Epoch 808/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 00808: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 0.9959\n",
            "Epoch 809/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 00809: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9959\n",
            "Epoch 810/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0336 - accuracy: 0.9984\n",
            "Epoch 00810: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0335 - accuracy: 0.9986 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 811/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0328 - accuracy: 0.9984\n",
            "Epoch 00811: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0325 - accuracy: 0.9986 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
            "Epoch 812/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 00812: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
            "Epoch 813/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 00813: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
            "Epoch 814/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0308 - accuracy: 1.0000\n",
            "Epoch 00814: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
            "Epoch 815/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
            "Epoch 00815: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "Epoch 816/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
            "Epoch 00816: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "Epoch 817/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 0.9970\n",
            "Epoch 00817: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0342 - accuracy: 0.9973 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 818/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0403 - accuracy: 0.9955\n",
            "Epoch 00818: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0394 - accuracy: 0.9959 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "Epoch 819/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0401 - accuracy: 0.9970\n",
            "Epoch 00819: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0392 - accuracy: 0.9973 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "Epoch 820/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9973\n",
            "Epoch 00820: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0407 - accuracy: 0.9973 - val_loss: 0.0628 - val_accuracy: 0.9919\n",
            "Epoch 821/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0498 - accuracy: 0.9926\n",
            "Epoch 00821: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0515 - accuracy: 0.9918 - val_loss: 0.0414 - val_accuracy: 0.9959\n",
            "Epoch 822/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0451 - accuracy: 0.9970\n",
            "Epoch 00822: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0441 - accuracy: 0.9973 - val_loss: 0.0466 - val_accuracy: 0.9959\n",
            "Epoch 823/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0551 - accuracy: 0.9940\n",
            "Epoch 00823: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0533 - accuracy: 0.9946 - val_loss: 0.0556 - val_accuracy: 0.9959\n",
            "Epoch 824/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0449 - accuracy: 0.9940\n",
            "Epoch 00824: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0475 - accuracy: 0.9932 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 825/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0559 - accuracy: 0.9911\n",
            "Epoch 00825: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0540 - accuracy: 0.9918 - val_loss: 0.0489 - val_accuracy: 0.9959\n",
            "Epoch 826/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0446 - accuracy: 0.9955\n",
            "Epoch 00826: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0440 - accuracy: 0.9959 - val_loss: 0.0455 - val_accuracy: 0.9959\n",
            "Epoch 827/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0398 - accuracy: 0.9985\n",
            "Epoch 00827: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0489 - accuracy: 0.9959 - val_loss: 0.0426 - val_accuracy: 0.9959\n",
            "Epoch 828/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0998 - accuracy: 0.9821\n",
            "Epoch 00828: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0983 - accuracy: 0.9823 - val_loss: 0.0802 - val_accuracy: 0.9837\n",
            "Epoch 829/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0571 - accuracy: 0.9969\n",
            "Epoch 00829: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0599 - accuracy: 0.9946 - val_loss: 0.0578 - val_accuracy: 0.9959\n",
            "Epoch 830/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0547 - accuracy: 0.9940\n",
            "Epoch 00830: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0579 - accuracy: 0.9918 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
            "Epoch 831/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0506 - accuracy: 0.9940\n",
            "Epoch 00831: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0498 - accuracy: 0.9946 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
            "Epoch 832/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0524 - accuracy: 0.9926\n",
            "Epoch 00832: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0512 - accuracy: 0.9932 - val_loss: 0.0474 - val_accuracy: 1.0000\n",
            "Epoch 833/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0495 - accuracy: 0.9970\n",
            "Epoch 00833: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0485 - accuracy: 0.9973 - val_loss: 0.0429 - val_accuracy: 0.9959\n",
            "Epoch 834/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0420 - accuracy: 1.0000\n",
            "Epoch 00834: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.0491 - val_accuracy: 0.9959\n",
            "Epoch 835/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 00835: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
            "Epoch 836/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
            "Epoch 00836: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.0388 - accuracy: 0.9986 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
            "Epoch 837/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0450 - accuracy: 0.9985\n",
            "Epoch 00837: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0443 - accuracy: 0.9986 - val_loss: 0.0465 - val_accuracy: 0.9919\n",
            "Epoch 838/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0411 - accuracy: 0.9984\n",
            "Epoch 00838: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0404 - accuracy: 0.9986 - val_loss: 0.0443 - val_accuracy: 0.9959\n",
            "Epoch 839/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0378 - accuracy: 1.0000\n",
            "Epoch 00839: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
            "Epoch 840/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 00840: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 841/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0374 - accuracy: 0.9985\n",
            "Epoch 00841: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0372 - accuracy: 0.9986 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "Epoch 842/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00842: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 843/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0456 - accuracy: 0.9940\n",
            "Epoch 00843: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0450 - accuracy: 0.9946 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
            "Epoch 844/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 00844: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 1.0000\n",
            "Epoch 845/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0348 - accuracy: 1.0000\n",
            "Epoch 00845: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
            "Epoch 846/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0390 - accuracy: 0.9985\n",
            "Epoch 00846: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0386 - accuracy: 0.9986 - val_loss: 0.0471 - val_accuracy: 0.9959\n",
            "Epoch 847/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0451 - accuracy: 0.9953\n",
            "Epoch 00847: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0449 - accuracy: 0.9959 - val_loss: 0.0528 - val_accuracy: 0.9959\n",
            "Epoch 848/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0390 - accuracy: 0.9985\n",
            "Epoch 00848: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0386 - accuracy: 0.9986 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
            "Epoch 849/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 00849: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "Epoch 850/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000\n",
            "Epoch 00850: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
            "Epoch 851/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 00851: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "Epoch 852/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 00852: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0355 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
            "Epoch 853/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 00853: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
            "Epoch 854/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 00854: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 855/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000\n",
            "Epoch 00855: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 856/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
            "Epoch 00856: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 857/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 00857: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0369 - accuracy: 0.9986 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
            "Epoch 858/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0485 - accuracy: 0.9955\n",
            "Epoch 00858: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0471 - accuracy: 0.9959 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "Epoch 859/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0421 - accuracy: 0.9970\n",
            "Epoch 00859: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0426 - accuracy: 0.9959 - val_loss: 0.0408 - val_accuracy: 0.9959\n",
            "Epoch 860/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0371 - accuracy: 0.9985\n",
            "Epoch 00860: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0367 - accuracy: 0.9986 - val_loss: 0.0481 - val_accuracy: 0.9959\n",
            "Epoch 861/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n",
            "Epoch 00861: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9959\n",
            "Epoch 862/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0324 - accuracy: 1.0000\n",
            "Epoch 00862: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0369 - val_accuracy: 0.9959\n",
            "Epoch 863/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0366 - accuracy: 0.9970\n",
            "Epoch 00863: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0363 - accuracy: 0.9973 - val_loss: 0.0391 - val_accuracy: 0.9959\n",
            "Epoch 864/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0406 - accuracy: 0.9970\n",
            "Epoch 00864: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0404 - accuracy: 0.9973 - val_loss: 0.0499 - val_accuracy: 0.9919\n",
            "Epoch 865/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0393 - accuracy: 0.9970\n",
            "Epoch 00865: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0387 - accuracy: 0.9973 - val_loss: 0.0670 - val_accuracy: 0.9878\n",
            "Epoch 866/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0515 - accuracy: 0.9940\n",
            "Epoch 00866: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0498 - accuracy: 0.9946 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
            "Epoch 867/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0374 - accuracy: 0.9985\n",
            "Epoch 00867: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0392 - accuracy: 0.9973 - val_loss: 0.0450 - val_accuracy: 0.9959\n",
            "Epoch 868/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0382 - accuracy: 0.9970\n",
            "Epoch 00868: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0379 - accuracy: 0.9973 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
            "Epoch 869/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000\n",
            "Epoch 00869: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
            "Epoch 870/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0380 - accuracy: 0.9970\n",
            "Epoch 00870: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0375 - accuracy: 0.9973 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "Epoch 871/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
            "Epoch 00871: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
            "Epoch 872/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 00872: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "Epoch 873/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 00873: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 874/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 00874: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 875/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000\n",
            "Epoch 00875: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
            "Epoch 876/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
            "Epoch 00876: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "Epoch 877/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
            "Epoch 00877: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 878/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 00878: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
            "Epoch 879/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000\n",
            "Epoch 00879: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
            "Epoch 880/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000\n",
            "Epoch 00880: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
            "Epoch 881/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 00881: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
            "Epoch 882/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
            "Epoch 00882: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
            "Epoch 883/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
            "Epoch 00883: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0310 - accuracy: 0.9986 - val_loss: 0.0357 - val_accuracy: 0.9959\n",
            "Epoch 884/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0318 - accuracy: 0.9984\n",
            "Epoch 00884: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0326 - accuracy: 0.9973 - val_loss: 0.0517 - val_accuracy: 0.9919\n",
            "Epoch 885/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0352 - accuracy: 0.9970\n",
            "Epoch 00885: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0350 - accuracy: 0.9973 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "Epoch 886/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0336 - accuracy: 0.9970\n",
            "Epoch 00886: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0333 - accuracy: 0.9973 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
            "Epoch 887/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0584 - accuracy: 0.9906\n",
            "Epoch 00887: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0591 - accuracy: 0.9891 - val_loss: 0.1059 - val_accuracy: 0.9756\n",
            "Epoch 888/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0580 - accuracy: 0.9896\n",
            "Epoch 00888: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0559 - accuracy: 0.9905 - val_loss: 0.0724 - val_accuracy: 0.9837\n",
            "Epoch 889/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0522 - accuracy: 0.9926\n",
            "Epoch 00889: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0512 - accuracy: 0.9932 - val_loss: 0.0528 - val_accuracy: 0.9959\n",
            "Epoch 890/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0462 - accuracy: 0.9926\n",
            "Epoch 00890: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0455 - accuracy: 0.9932 - val_loss: 0.0662 - val_accuracy: 0.9878\n",
            "Epoch 891/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0936 - accuracy: 0.9828\n",
            "Epoch 00891: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0897 - accuracy: 0.9823 - val_loss: 0.0486 - val_accuracy: 0.9959\n",
            "Epoch 892/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0473 - accuracy: 0.9911\n",
            "Epoch 00892: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0462 - accuracy: 0.9918 - val_loss: 0.0620 - val_accuracy: 0.9919\n",
            "Epoch 893/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0520 - accuracy: 0.9906\n",
            "Epoch 00893: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0500 - accuracy: 0.9918 - val_loss: 0.0454 - val_accuracy: 0.9959\n",
            "Epoch 894/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0522 - accuracy: 0.9896\n",
            "Epoch 00894: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0529 - accuracy: 0.9891 - val_loss: 0.0446 - val_accuracy: 0.9959\n",
            "Epoch 895/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0407 - accuracy: 0.9985\n",
            "Epoch 00895: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0406 - accuracy: 0.9986 - val_loss: 0.0433 - val_accuracy: 0.9959\n",
            "Epoch 896/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0423 - accuracy: 0.9985\n",
            "Epoch 00896: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0440 - accuracy: 0.9959 - val_loss: 0.0423 - val_accuracy: 0.9959\n",
            "Epoch 897/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0465 - accuracy: 0.9955\n",
            "Epoch 00897: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0468 - accuracy: 0.9959 - val_loss: 0.0419 - val_accuracy: 0.9959\n",
            "Epoch 898/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0377 - accuracy: 0.9985\n",
            "Epoch 00898: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0376 - accuracy: 0.9986 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 899/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 00899: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0359 - val_accuracy: 1.0000\n",
            "Epoch 900/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
            "Epoch 00900: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "Epoch 901/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 00901: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
            "Epoch 902/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
            "Epoch 00902: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 0.9959\n",
            "Epoch 903/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 00903: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
            "Epoch 904/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 1.0000\n",
            "Epoch 00904: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 0.9959\n",
            "Epoch 905/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0328 - accuracy: 1.0000\n",
            "Epoch 00905: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9959\n",
            "Epoch 906/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000\n",
            "Epoch 00906: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9959\n",
            "Epoch 907/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 00907: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9959\n",
            "Epoch 908/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0314 - accuracy: 1.0000\n",
            "Epoch 00908: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 0.9959\n",
            "Epoch 909/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 00909: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9959\n",
            "Epoch 910/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 00910: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 0.9959\n",
            "Epoch 911/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 00911: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
            "Epoch 912/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0298 - accuracy: 1.0000\n",
            "Epoch 00912: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "Epoch 913/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 00913: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 0.9959\n",
            "Epoch 914/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
            "Epoch 00914: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 0.9959\n",
            "Epoch 915/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0307 - accuracy: 0.9985\n",
            "Epoch 00915: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0305 - accuracy: 0.9986 - val_loss: 0.0317 - val_accuracy: 0.9959\n",
            "Epoch 916/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0368 - accuracy: 0.9970\n",
            "Epoch 00916: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0361 - accuracy: 0.9973 - val_loss: 0.0480 - val_accuracy: 0.9919\n",
            "Epoch 917/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0339 - accuracy: 0.9985\n",
            "Epoch 00917: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0335 - accuracy: 0.9986 - val_loss: 0.0401 - val_accuracy: 0.9919\n",
            "Epoch 918/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0341 - accuracy: 0.9970\n",
            "Epoch 00918: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 0.9973 - val_loss: 0.0328 - val_accuracy: 1.0000\n",
            "Epoch 919/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0410 - accuracy: 0.9970\n",
            "Epoch 00919: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0430 - accuracy: 0.9959 - val_loss: 0.0445 - val_accuracy: 0.9959\n",
            "Epoch 920/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0379 - accuracy: 0.9985\n",
            "Epoch 00920: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0380 - accuracy: 0.9986 - val_loss: 0.0410 - val_accuracy: 0.9959\n",
            "Epoch 921/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 0.9985\n",
            "Epoch 00921: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0349 - accuracy: 0.9986 - val_loss: 0.0337 - val_accuracy: 0.9959\n",
            "Epoch 922/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0382 - accuracy: 0.9970\n",
            "Epoch 00922: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0387 - accuracy: 0.9973 - val_loss: 0.0383 - val_accuracy: 0.9959\n",
            "Epoch 923/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0339 - accuracy: 0.9984\n",
            "Epoch 00923: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0349 - accuracy: 0.9986 - val_loss: 0.0383 - val_accuracy: 0.9959\n",
            "Epoch 924/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 00924: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
            "Epoch 925/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0326 - accuracy: 0.9985\n",
            "Epoch 00925: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0323 - accuracy: 0.9986 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "Epoch 926/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 00926: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0342 - val_accuracy: 0.9959\n",
            "Epoch 927/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0306 - accuracy: 0.9985\n",
            "Epoch 00927: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0307 - accuracy: 0.9986 - val_loss: 0.0349 - val_accuracy: 0.9959\n",
            "Epoch 928/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 00928: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "Epoch 929/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
            "Epoch 00929: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "Epoch 930/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000\n",
            "Epoch 00930: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
            "Epoch 931/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
            "Epoch 00931: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
            "Epoch 932/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 00932: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "Epoch 933/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 00933: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
            "Epoch 934/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0287 - accuracy: 0.9985\n",
            "Epoch 00934: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0311 - accuracy: 0.9973 - val_loss: 0.0358 - val_accuracy: 0.9959\n",
            "Epoch 935/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
            "Epoch 00935: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 0.9959\n",
            "Epoch 936/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0285 - accuracy: 0.9985\n",
            "Epoch 00936: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0283 - accuracy: 0.9986 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
            "Epoch 937/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0373 - accuracy: 0.9970\n",
            "Epoch 00937: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0369 - accuracy: 0.9973 - val_loss: 0.0690 - val_accuracy: 0.9878\n",
            "Epoch 938/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0366 - accuracy: 0.9953\n",
            "Epoch 00938: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0358 - accuracy: 0.9959 - val_loss: 0.0442 - val_accuracy: 0.9959\n",
            "Epoch 939/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 00939: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 0.9959\n",
            "Epoch 940/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0411 - accuracy: 0.9926\n",
            "Epoch 00940: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0403 - accuracy: 0.9932 - val_loss: 0.0460 - val_accuracy: 0.9919\n",
            "Epoch 941/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0330 - accuracy: 0.9970\n",
            "Epoch 00941: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0327 - accuracy: 0.9973 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 942/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0312 - accuracy: 0.9985\n",
            "Epoch 00942: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0312 - accuracy: 0.9986 - val_loss: 0.0334 - val_accuracy: 0.9959\n",
            "Epoch 943/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 00943: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0316 - accuracy: 0.9986 - val_loss: 0.0354 - val_accuracy: 0.9959\n",
            "Epoch 944/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 00944: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
            "Epoch 945/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000\n",
            "Epoch 00945: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 0.9959\n",
            "Epoch 946/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0314 - accuracy: 0.9984\n",
            "Epoch 00946: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0309 - accuracy: 0.9986 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 947/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000\n",
            "Epoch 00947: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
            "Epoch 948/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0273 - accuracy: 1.0000\n",
            "Epoch 00948: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "Epoch 949/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0326 - accuracy: 0.9970\n",
            "Epoch 00949: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0321 - accuracy: 0.9973 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
            "Epoch 950/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0304 - accuracy: 0.9985\n",
            "Epoch 00950: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0304 - accuracy: 0.9986 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "Epoch 951/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 00951: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "Epoch 952/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0266 - accuracy: 1.0000\n",
            "Epoch 00952: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "Epoch 953/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
            "Epoch 00953: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
            "Epoch 954/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0270 - accuracy: 1.0000\n",
            "Epoch 00954: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
            "Epoch 955/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
            "Epoch 00955: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 0.9959\n",
            "Epoch 956/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0262 - accuracy: 1.0000\n",
            "Epoch 00956: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9959\n",
            "Epoch 957/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0253 - accuracy: 1.0000\n",
            "Epoch 00957: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 0.9959\n",
            "Epoch 958/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000\n",
            "Epoch 00958: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9959\n",
            "Epoch 959/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0262 - accuracy: 1.0000\n",
            "Epoch 00959: loss did not improve from 0.02512\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9959\n",
            "Epoch 960/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0252 - accuracy: 1.0000\n",
            "Epoch 00960: loss improved from 0.02512 to 0.02509, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9959\n",
            "Epoch 961/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0244 - accuracy: 1.0000\n",
            "Epoch 00961: loss improved from 0.02509 to 0.02454, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 0.9959\n",
            "Epoch 962/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0253 - accuracy: 0.9985\n",
            "Epoch 00962: loss did not improve from 0.02454\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0253 - accuracy: 0.9986 - val_loss: 0.0448 - val_accuracy: 0.9959\n",
            "Epoch 963/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0272 - accuracy: 0.9985\n",
            "Epoch 00963: loss did not improve from 0.02454\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0269 - accuracy: 0.9986 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
            "Epoch 964/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.0243 - accuracy: 1.0000\n",
            "Epoch 00964: loss did not improve from 0.02454\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
            "Epoch 965/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0243 - accuracy: 1.0000\n",
            "Epoch 00965: loss improved from 0.02454 to 0.02434, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
            "Epoch 966/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0239 - accuracy: 1.0000\n",
            "Epoch 00966: loss improved from 0.02434 to 0.02381, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
            "Epoch 967/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000\n",
            "Epoch 00967: loss did not improve from 0.02381\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
            "Epoch 968/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0229 - accuracy: 1.0000\n",
            "Epoch 00968: loss improved from 0.02381 to 0.02285, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
            "Epoch 969/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0228 - accuracy: 1.0000\n",
            "Epoch 00969: loss improved from 0.02285 to 0.02274, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
            "Epoch 970/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0231 - accuracy: 1.0000\n",
            "Epoch 00970: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
            "Epoch 971/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0400 - accuracy: 0.9940\n",
            "Epoch 00971: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0440 - accuracy: 0.9932 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
            "Epoch 972/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0363 - accuracy: 0.9955\n",
            "Epoch 00972: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0381 - accuracy: 0.9946 - val_loss: 0.0397 - val_accuracy: 0.9959\n",
            "Epoch 973/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0287 - accuracy: 1.0000\n",
            "Epoch 00973: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
            "Epoch 974/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0287 - accuracy: 0.9985\n",
            "Epoch 00974: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0286 - accuracy: 0.9986 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
            "Epoch 975/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0256 - accuracy: 1.0000\n",
            "Epoch 00975: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0269 - accuracy: 0.9986 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
            "Epoch 976/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0290 - accuracy: 0.9985\n",
            "Epoch 00976: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0533 - accuracy: 0.9959 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "Epoch 977/1000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1170 - accuracy: 0.9734\n",
            "Epoch 00977: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1174 - accuracy: 0.9714 - val_loss: 0.0635 - val_accuracy: 0.9878\n",
            "Epoch 978/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0595 - accuracy: 0.9926\n",
            "Epoch 00978: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0621 - accuracy: 0.9905 - val_loss: 0.0465 - val_accuracy: 0.9959\n",
            "Epoch 979/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0517 - accuracy: 0.9911\n",
            "Epoch 00979: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0502 - accuracy: 0.9918 - val_loss: 0.0603 - val_accuracy: 0.9959\n",
            "Epoch 980/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0435 - accuracy: 0.9940\n",
            "Epoch 00980: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0425 - accuracy: 0.9946 - val_loss: 0.0391 - val_accuracy: 0.9959\n",
            "Epoch 981/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0350 - accuracy: 0.9985\n",
            "Epoch 00981: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0347 - accuracy: 0.9986 - val_loss: 0.0526 - val_accuracy: 0.9959\n",
            "Epoch 982/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0324 - accuracy: 1.0000\n",
            "Epoch 00982: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 0.9959\n",
            "Epoch 983/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 00983: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9959\n",
            "Epoch 984/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0336 - accuracy: 0.9985\n",
            "Epoch 00984: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0364 - accuracy: 0.9973 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
            "Epoch 985/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0418 - accuracy: 0.9955\n",
            "Epoch 00985: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0409 - accuracy: 0.9959 - val_loss: 0.0492 - val_accuracy: 0.9959\n",
            "Epoch 986/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0423 - accuracy: 0.9970\n",
            "Epoch 00986: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0413 - accuracy: 0.9973 - val_loss: 0.0618 - val_accuracy: 0.9919\n",
            "Epoch 987/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0368 - accuracy: 0.9970\n",
            "Epoch 00987: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0365 - accuracy: 0.9973 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
            "Epoch 988/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0395 - accuracy: 0.9970\n",
            "Epoch 00988: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0442 - accuracy: 0.9959 - val_loss: 0.0383 - val_accuracy: 0.9959\n",
            "Epoch 989/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0432 - accuracy: 0.9970\n",
            "Epoch 00989: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0443 - accuracy: 0.9959 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
            "Epoch 990/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0344 - accuracy: 0.9985\n",
            "Epoch 00990: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0341 - accuracy: 0.9986 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
            "Epoch 991/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0350 - accuracy: 0.9985\n",
            "Epoch 00991: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0347 - accuracy: 0.9986 - val_loss: 0.0346 - val_accuracy: 0.9959\n",
            "Epoch 992/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0338 - accuracy: 0.9985\n",
            "Epoch 00992: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0335 - accuracy: 0.9986 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "Epoch 993/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 00993: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 994/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 00994: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
            "Epoch 995/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0339 - accuracy: 0.9970\n",
            "Epoch 00995: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0335 - accuracy: 0.9973 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
            "Epoch 996/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 0.9985\n",
            "Epoch 00996: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0385 - accuracy: 0.9973 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
            "Epoch 997/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0360 - accuracy: 0.9985\n",
            "Epoch 00997: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0355 - accuracy: 0.9986 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "Epoch 998/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0329 - accuracy: 0.9985\n",
            "Epoch 00998: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0328 - accuracy: 0.9986 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
            "Epoch 999/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000\n",
            "Epoch 00999: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 1.0000\n",
            "Epoch 1000/1000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
            "Epoch 01000: loss did not improve from 0.02274\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONp4QSBfQQG0",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGBB-gJfTl4h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a67f93ef-de19-445a-bd6e-d188ad24c336"
      },
      "source": [
        "score = Model.evaluate(X_train, Y_train)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 0s 7ms/step - loss: 0.0287 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5zEPcF3I9Hz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d5d507c-7c9d-436d-e369-09b1c0d89e1f"
      },
      "source": [
        "score = Model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0291 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQW8rjpGJHUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pred=Model.predict(X_test)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQMFS78EJgyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b3e4934b-8bff-4783-8658-c31a370af366"
      },
      "source": [
        "Pred"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.68134073e-09, 5.25928086e-11, 5.46700324e-11, ...,\n",
              "        7.71565616e-01, 8.71616357e-05, 4.54742303e-06],\n",
              "       [5.06911943e-11, 5.20724059e-17, 2.85400745e-07, ...,\n",
              "        2.65912314e-12, 7.78120279e-01, 2.07833662e-14],\n",
              "       [5.95332267e-07, 7.41830490e-07, 1.73864496e-08, ...,\n",
              "        4.54492539e-01, 9.06203127e-07, 1.08237209e-05],\n",
              "       ...,\n",
              "       [1.56573861e-07, 3.44979071e-07, 3.86641860e-01, ...,\n",
              "        3.84031367e-08, 4.99448021e-08, 6.93729362e-13],\n",
              "       [1.91495033e-07, 7.53485918e-01, 1.24115175e-07, ...,\n",
              "        2.53645931e-06, 6.98523905e-09, 2.22882386e-06],\n",
              "       [5.07813091e-14, 1.79454021e-12, 1.35435105e-14, ...,\n",
              "        9.94049311e-01, 2.74002265e-09, 1.31444740e-08]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5UbFqk3Jjin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "33717b99-e545-46da-d422-a81522e720e2"
      },
      "source": [
        "Y_test"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lumQfISRJmF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57c61f36-2700-4669-debe-341690d840ce"
      },
      "source": [
        "len(Pred)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3WmRMWhNxVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image(ind,images_f,images_f_2,Model):\n",
        "  cv2_imshow(images_f[ind])\n",
        "  image_test=images_f_2[ind]\n",
        "  print(\"Label actual:  \" + Exp[labels[ind]]  )\n",
        "  pred_1=Model.predict(np.array([image_test]))\n",
        "  #print(pred_1)\n",
        "  pred_class=Exp[int(np.argmax(pred_1))]\n",
        "  print(\"Predicted Label: \"+ pred_class)\n",
        "\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-k9IW1dOgyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "c11fbc93-c38e-4151-bacc-038eda9af80a"
      },
      "source": [
        "test_image(72,images_f,images_f_2,Model)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F20B6AAC8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  fear\n",
            "Predicted Label: fear\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcleDEVuNpvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "16c49349-c9f2-49e4-c454-2d8f12281efb"
      },
      "source": [
        "test_image(36,images_f,images_f_2,Model)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F20B6AB70>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  fear\n",
            "Predicted Label: fear\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcwbyrEgO3YK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "474c3844-b690-44b2-fa39-b5d3248d5534"
      },
      "source": [
        "test_image(122,images_f,images_f_2,Model)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F20B6A9B0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  contempt\n",
            "Predicted Label: contempt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgLf7NGWP7bm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "aaa1510e-96fa-4094-8ddf-0927a473dfda"
      },
      "source": [
        "test_image(132,images_f,images_f_2,Model)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F21DDE860>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  happy\n",
            "Predicted Label: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6bdhwOhP-li",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "ce27fab1-723b-41c9-971a-d7fd848e837d"
      },
      "source": [
        "test_image(147,images_f,images_f_2,Model)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F21D81BE0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  happy\n",
            "Predicted Label: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k4ELPe5QBhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "7813bd02-6f09-4677-dc76-8bdef4f40c68"
      },
      "source": [
        "test_image(369,images_f,images_f_2,Model)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F207C3550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  anger\n",
            "Predicted Label: anger\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpz-wzUCQGX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "b7410df4-49a3-4658-a0bf-4a9db78c6a05"
      },
      "source": [
        "test_image(502,images_f,images_f_2,Model)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F205B51D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  surprise\n",
            "Predicted Label: surprise\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA8uKPvGQJfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "3382ea52-1fa1-41e4-ecfa-e48cd36e2c2b"
      },
      "source": [
        "test_image(800,images_f,images_f_2,Model)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAIAAADYYG7QAAAOsUlEQVR4nHVY2W8b5Rf9Zp9vVi9jJ17TBDlNKFBR0oWlIEShL7zxH/D3sTzAAxVCQCqERAhSaVWaNkCI28RuHHvs2fffw0mn/W3zlNjjmfvde+45516m2+0WRZFlGSGkKAqWZVmW5XmeZVlCCCGEZdmiKBiGYVk2y7Isy+I4TpIENwuCIMuyJEksyyZJkqZpGIZxHBdFwfO8LMuCIPA8z3EcPsmyLM9z/B3HcZ7nhBCGYfBhURTP72BZFm/lOI4Q8t+34mUsy7bb7ZdffnljY+P8+fP1et00zWq1yrJsHMdhGNq2fXx8fO/evZ2dnYcPH06nU0EQVFUVBKF8C57McRzHcWma5nnOcVySJHme82UOCCEcxyExDMNkWYZf5nkex3Ecx7quv/vuuzdu3Pjggw/W1tYEQSD/5yqKwrbtx48f7+zsfPvttzs7O9PpVJZlSinP86Io5nme5zlOy7Is/mVZNk1TZnl5mWEYxI5QUDUkJk3TKIpEUXzttdc+/PDDq1evdjqddruNLL54AMSRpinKOp/PPc/zPG82m+3u7n799df3798nhMiyLIoix3EMw5QHTtM0TVNkgU/TFGkEUFApQghuiuO40Whcv3798uXLjUYjyzLHcU5PTzVN43leEAREhidkWZYkSRzHWZbxPA/EEEI2NjZ0Xd/e3v7xxx9d16WUiqIoCAKqhrAQGcMwPP4pj4gTJ0kSRVGapoPB4O23315bW5MkCVFGUbRYLNI01TQtyzLgoCgKnAQ3oARlClmWrdfrH330UbVa/eabbyaTSZqmZSuU9QGYeMQBDOG7LMvw0MFg8NZbb7XbbXSKJEmyLBdF4bouIQRtwjy7SuyX4ECqkHiO4yil165dsyzr1q1bDx48wFeAsyiK5aN4gAYB4YhxHBNCzp8/f+XKlWaziableR4RS5IkCALLskEQvNgBJZ7KxkHD4n1FUXAcJwjChQsXLMv66quvdnd3Pc+L45hSWtJKURR82RpIMo71yiuvvPHGG41Gg1KKYqEKSZKgOcv64lmAJNgLuAHC0jTF/UmSgJbCMGw0Gjdv3nRd9969ewi0bJE8z3m0d1msLMv6/f7Fixer1SqlVHh2iaIoSVKWZUEQUErjOMa5CSFAG+hUkiRJkvCCoihEUaSUgurwHLzVMIyLFy8+fvx4sVhEUcQwDFDxHNSEkCzLwjA0DOOll17SNE1V1TIgVVUlSSpbAwVCH6HPkYDyfVmWsSwriqIsy2BwdHtRFHEcI/Tl5eV+v7+3t4csApFpmp6BGmSQ53mlUtE0DSGrqqrrOsdxiqLg6IIgIP9hGOZ5rqoqugNQQG6CIADf4DCAoKZphBDXdeNnV61Wa7fb+/v7vu8TQnCnKIo8z/MlNYmiWKlUcILya1EUWZallOq6TgjxfT/P8zAM0Wu6rsuyjGyHYTifz6Mo4nkeuhbHcRRFgFEURYIgWJZVr9dBob1eT5Zlz/OAFo7j4jjmy/ZL09QwDFmWAU9BELIsOzk5EUVR13W8EorjOE4QBOgRnudN04zj2Pf9MAxnsxkylGXZeDweDoee53EcV6/X8XBVVVutlmEYaZqurKw0Go2TkxOUErTEA87oUqBEUZRKpUIIOTg4sG0bpURFDMOo1WqiKKJqPM+7rqsoCtoKAJ/NZnEcHx8f//PPPyDAZrMpSRIins/nR0dH3W631Wq1Wq21tbU//vgDLQwLwJcCRAhRVVUURUVRPM87ODjwfR+JyfPc9310k67rrVarXq/LshzH8XQ6zfNcFEWGYaB6hmFMp1NJktrtdlEUzWaz2WwKghAEQRRFKMh4PJZludFoXLp06fbt26D+JElYluXzZxfDMIimKIrhcBhFka7ruq4Dy3EcQy+zLBuNRvP53DAM0zTDMIyiqNlsZlm2WCwYhvF9fz6fZ1lWr9c7nU6v1zNNEwEtFgvHcUqaoJRubGz0+30Qd5IklNLnAQEruq6HYUgIWVpaMgwD0YD4u93uYrE4PT1dLBZwF4Ig9Ho9QgillGVZ27afPn16eHiY57miKEEQ4NCyLPu+PxwOF4sF2ASdK4pip9N58803//7771IBn4urKIqgHzipoiiCIEA/W5bV7/dN0zw6OgLHq6o6n8/RaK7rep6nKAoIllJaVnk4HP7222+GYURRVK/X+/3+o0ePms1mt9sF8CVJun79+nfffXd0dATqPwuI4zhJkqrVKqjF9/0gCBzHYRimVqsBpHmen56ezudzXdcRvaIojUajUqkoimIYRrVa1XXdsixJkn7//ffFYnHhwoXPP/88yzLf93u93vb2dqfTaTQahJBarYaUrK6ubm5uTiYT8BNfWlVd1yuVCpKfpqnv+2ixvb29oigODw8ppVEU3bx5czwedzqd1dVV27ZVVW2324PBYHl52bZtx3Esy+J5PggCRVEEQfj0009BS4SQMAzDMKxWq+fOnev3+6IohmEoSdKlS5e2t7fPGq0sGagCJeM47ujoyHEc27aTJBkMBp1O59y5c7VabWNj4/T0NE1TSqksy+12u9Vq6bqOpl1ZWTk+PtY0bWtrazgcTqfTWq0GFwA9sSyr0+mYpgkq8jwvz/PXX3/dMIzJZMJxHA+BFEWxVqvBNGmaVqlUDMOI49jzPCgRaoGCrq6uiqI4Go2Q1CRJ0F/QhyRJoMTnz5/Pssx13SAIUAHDMNC2Z9aH5ymlvu8vLS11u92TkxOGYc4C4nm+Wq0qiqLruqIoqqr2ej3HcRzH8X0/yzIwjaZplmVBzMMwhPCBIERRhI0siiJJEtM0NU37n4MAmA8+B56JUtpqtXZ3dwkhPPyUJEmKosCKQL9EUWw0GisrK0gbAIHWw+MEQfB9n1Jaq9Wgx67rguLCMEQHgVdfjKZ0Xb7vp2mKjKZpury8DH1lNU1L0xS+DlYaTQsx1zQNcqGqKjyhIAimaTqOs7+/PxqNZFnWNA2nkmXZsiyWZUej0c8//7y3twcsv+hw4ji2bRvpgeJisOx2u5qmxXHMv//++19++aVhGCCYOI5d1wWuPc9DmSF+sHY8z9+5c2d7e7vX61UqFVQtz3PoMSYQSmmSJF988cWTJ0+2trYgfygWQknT1HEc5Bt2ABL73nvv8VevXn3y5Ilt26BB9CHyhPLBwsLWhGF4dHR069at9fX1GzdulOMmRgtUhFKqKIqmaffv3//hhx+SJNnc3Oz1enBzSZJgiMDAtFgs4GQopZ988snW1hbPcdzHH3+8s7MTBIHruqh9EASgSsxWruvC0c1ms4cPH2ZZRimdzWZwM3EcK4rCcVwURcPhkOd5RVHgWxRFOTg4QI5hmzBHgHjTNEUWRqPR1taW53lhGPJ5nmuaduHChd3d3fl8zvM8LBxQhY7F8BtFked5hBBJkn755ZcHDx6gCpubm0tLSxCTu3fv/vnnn7PZrFqtrq6uvvPOO6enp47jnJycGIbx4rCAJCVJEgSB7/tovaIozvwQpbTf75+enrquiw53HAe/932/nPfSNOV5fn19/fbt23fu3JlMJqZpjsfjVqtFKQ3D8OHDh3/99RfDMGtra71eb319PQzD0WiU5/l8PseUAmiXuHZddzab+b6/vr5+9+7dM7tjGMa1a9c+++wz9BEABF+LDUGJEl3X6/X6YDCYTCYowcHBwfHxcblwWVtbGwwG165dQ29WKhWO46bTqed5pZmEMYJWeJ7XaDRs24Yj5WFmt7a29vf3F4tF6f1gapMkUVUVHcTzvKqqoKXBYDCfz0ejkeM4OCshpNFoGIZhWda5c+dAd8gH/DiIB5WKoggTGVgUBmE8HidJwuOLMAzH43Gj0ZhMJiUPZc8uzECgBmyZ4K0sy0rTtF6vo6ur1SpYDWmO43g8HsMWIxlJkoD38S+OXRQFXNvZaAWudF2X47iVlZXxeIzxIAzDEjphGIIYMaJgluA4zrIsiBTHcbIsm6bJMEwQBBiSKKXz+TwIAihguYIqt3XlxseyLMMwznQCXmw4HLquu7y8jFkEmzJsiaIoopSqqhrHcRAEtm0jbbZtwyvChOi6jlUf4DKfzyVJMk0zz/NqtQqpxsRdQrt0PuAzvI5HpI7jSJIUx3G9Xj85OUGZywUDIcTzPEgYWPXw8DAIAtM0S0uKJoAOep7nui7We5ZlXb58GXBBf8E+Q8j+o3ZFUfDgWWgN3OrTp0/xS8QuSRJcNuA5nU6///57z/OuXLnS6XQsy+p2u3CJ8/lclmUYEt/3f/31159++ikIgoODA0IICoo5B1UreYQ821adlaxcJ8JZAoNQHBAGxvIoijBRXLx4sd1ub25uiqK4trYGe1SqFcMwtm2DfF999dXpdHr//n3btuEXcA+wCDjCo5aleJ4hRKZpGmQBrg2NigZGQJTSpaUlhmEePXoEikrTtN1uY1wMgmA6nT558gRMgwG3Xq8nSYIRFvvkctSBR3ixamegLhdhkiTpuo6tG+7GVgQahCLOZrOnT59GUcSy7N7eHo6O9oFkFkUBC4WlMVC8WCzKUoD8zgYx/vkW7wzUAHy5pMYeCNzFMAzkBusHSZLyPMcmxHXd6XQKjSxXWOj/Wq3W6XQ0TRNF0XEcBAqTCZYCgMD7xbPrrJrwkaW5RAURYjlAIlA4OOSA5/l6va5pGvqlRCvHcaqqYqhFLZAVwAVjLnm2DG2325RSaGoZwL+VDB8BDaW/hDVDbsqhGxYA2MLKEX/AC4DZywcii+VhQPemaZqmiWjK/V2e52dzGS6kCmuhshB5noNF8FZAD6wtyzJ2ZNATZLHUZpS79BsYP2D9OI5rNpvlscudYlEUZzJevgzLKARbcnzxbNmLXVYcx1gjlaYAwomVEroaPr0URMdxFosFdlOQYUopngn1eN72CDMMQyzh8Ahd14MgALyQknKvhVzCTJY4xecwlnDZhBBEmaap67rlRMUwDFSvjKN87FmXHR4eHh0d7e/vN5vNfr+PaaZarUZRNJlMkiQBAEudLy/P88CH0BlgX5ZlGBKcE4yPzRoUl2VZLMSQwhexi+tfMsWGqIRXxX0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F9F207C3748>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  disgust\n",
            "Predicted Label: disgust\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyx5ao4dQgMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ5pvdm4QNtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "7dcaaa0f-02cc-4541-eb05-86b9d88aeb9e"
      },
      "source": [
        "plt.plot(History.history['loss'])\n",
        "plt.plot(History.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n",
        "                        wspace=0.35)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UycVjuzdQlW8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "0f23d596-c1a7-4a93-ca1d-7805ab4e3831"
      },
      "source": [
        "plt.plot(History.history['accuracy'])\n",
        "plt.plot(History.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n",
        "                        wspace=0.35)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pAhlICdRUgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "\n",
        "from sklearn.metrics import classification_report "
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je6eHsWERabS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=0\n",
        "Y_test_l=[]\n",
        "Pred_l=[]\n",
        "while(i<len(Pred)):\n",
        "  Y_test_l.append(int(np.argmax(Y_test[i])))\n",
        "  Pred_l.append(int(np.argmax(Pred[i])))\n",
        "  i+=1\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGRVFqYEQ0fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "report=classification_report(Y_test_l, Pred_l)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXLt0q86SiI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7780a5a2-c94f-49fb-e364-8a974310639a"
      },
      "source": [
        "print(report)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        18\n",
            "           2       1.00      1.00      1.00        53\n",
            "           3       1.00      1.00      1.00        30\n",
            "           4       1.00      1.00      1.00        70\n",
            "           5       1.00      1.00      1.00        36\n",
            "           6       1.00      1.00      1.00        20\n",
            "\n",
            "    accuracy                           1.00       246\n",
            "   macro avg       1.00      1.00      1.00       246\n",
            "weighted avg       1.00      1.00      1.00       246\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LcBCMAESlya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "results = confusion_matrix(Y_test_l, Pred_l)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5x8_HqWSry3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "7f74443e-5c4a-4e76-cfaa-500f53887eaa"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(results, annot=True)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9f20e81198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}